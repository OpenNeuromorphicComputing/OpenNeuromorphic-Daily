# ðŸ§  Open Neuromorphic - Daily ArXiv

**Automated Daily Update** | Last Run: 2026-02-17 08:43 UTC

Papers are automatically categorized by topic and sorted by date.

## ðŸ›  Hardware & Materials

### [Coupled integrated photonic quantum memristors using a single photon source made of a colour center](http://arxiv.org/abs/2602.14736v1)
**2026-02-16** | *Alessio Baldazzi, Roy Philip George Konnoth Ancel, Sebastiano Guaraldo et al.*

> Photonic quantum memristors provide a measurement-induced route to nonlinear and history-dependent quantum dynamics. Experimental demonstrations have so far focused on isolated devices or simple cascaded devices configurations. Here, we experimentally realize and characterize a network of two coupled photonic quantum memristors with crossed feedback, implemented on a silicon nitride photonic integrated circuit and fed by a room-temperature single-photon source based on a silicon-vacancy color center SiV$^-$ in a nanodiamond. Each memristor consists of an integrated Mach-Zehnder interferometer whose transfer function is adaptively updated by photon detection events on another memristor, thus generating novel non-Markovian input-output dynamics with an enhanced memristive behaviour compared to single devices. In particular, we report inter-memristor input-output hysteresis curves exhibiting larger form factors and displaying self-intersecting loops, respectively revealing marked bistability and topologically non-trivial memory dynamics. Furthermore, numerical simulations show how these features emerge from the interplay between memory depth and relative input phase, for both intra- and inter-memristor input-output relations. Our results establish coupled integrated photonic quantum memristors as scalable nonlinear building blocks and highlight their potential for implementing compact quantum neuromorphic and reservoir computing architectures.

### [Pattern recognition with superconducting wirelet neurons](http://arxiv.org/abs/2602.14330v1)
**2026-02-15** | *Khalil Harrabi, Leonardo Cadorim, Milorad Milosevic*

> Neuromorphic computing aims to reproduce the energy efficiency and adaptability of biological intelligence in hardware. Superconducting devices are an attractive platform due to their ultra-low dissipation and fast switching dynamics. Here we introduce a shunted superconducting wirelet as an artificial neuron, representing the simplest possible superconducting neuron implementation. This minimal design, a single superconducting channel with a resistive shunt, enables straightforward fabrication, electronic control, and high scalability. The neuron exhibits spiking voltage behavior driven by the interplay of resistive switching and relaxation, with key properties such as threshold, firing frequency, and refractory time tunable via applied current, temperature, and shunt resistance. We further show that the resulting temporal voltage signals can be incorporated into a training algorithm to achieve accurate pattern recognition, demonstrating suitability for neuromorphic tasks. Finally, we discuss on-chip training using similar wirelets with gated synaptic weights, establishing a scalable, energy-efficient building block for cryogenic artificial intelligence hardware, integrable with other emergent superconducting technologies.

### [Disorder-driven stochastic dynamics in Mott resistive-switching systems](http://arxiv.org/abs/2602.14173v1)
**2026-02-15** | *David J. Alspaugh, Lorenzo Fratino, Nareg Ghazikhanian et al.*

> Controlled disorder in correlated materials provides a new route to emergent stochastic dynamics in neuromorphic hardware. Here we show that focused ion beam irradiation in VO$_{2}$- and V$_{2}$O$_{3}$-based resistive-switching oscillators induces a transition from regular periodic oscillations to strongly irregular stochastic firing, while simultaneously reducing the required switching energy by orders of magnitude. Under an applied electric field, these materials undergo a volatile insulator-to-metal transition characterized by the formation of percolating metallic filaments within an insulating bulk. Using numerical simulations based on the Mott resistor network, we demonstrate that defect-induced modifications to filament nucleation and stability drive these devices into stochastic oscillatory regimes. These results are validated by experimental measurements on irradiated VO$_{2}$ and V$_{2}$O$_{3}$ devices.

### [Implementation and Performance Evaluation of CMOS-integrated Memristor-driven Flip-flop Circuits](http://arxiv.org/abs/2602.13825v1)
**2026-02-14** | *Paras Tiwari, Narendra Singh Dhakad, Shalu Rani et al.*

> In this work, we report implementation and performance evaluation of memristor-driven fundamental logic gates, including NOT, AND, NAND, OR, NOR, and XOR, and novel and optimized design of the sequential logic circuits, such as D flip-flop, T-flip-flop, JK-flip-flop, and SR-flip-flop. The design, implementation, and optimization of these logic circuits were performed in SPECTRE in Cadence Virtuoso and integrated with 90 nm CMOS technology node. Additionally, we discuss an optimized design of memristor-driven logic gates and sequential logic circuits, and draw a comparative analysis with the other reported state-of-the-art work on sequential circuits. Moreover, the utilized memristor framework was experimentally pre-validated with the experimental data of Y2O3-based memristive devices, which shows significantly low values of variability during switching in both device-to-device (D2D) and cycle-to-cycle (C2C) operation. The performance metrics were calculated in terms of area, power, and delay of these sequential circuits and were found to be reduced by more than ~24%, 60%, and 58%, respectively, as compared to the other state-of-the-art work on sequential circuits. Therefore, the implemented memristor-based design significantly improves the performance of various logic designs, which makes it more area and power-efficient and shows the potential of memristor in designing various low-power, low-cost, ultrafast, and compact circuits.

### [The More the Merrier: Running Multiple Neuromorphic Components On-Chip for Robotic Control](http://arxiv.org/abs/2602.13747v1)
**2026-02-14** | *Evan Eames, Priyadarshini Kannan, Ronan Sangouard et al.*

> It has long been realized that neuromorphic hardware offers benefits for the domain of robotics such as low energy, low latency, as well as unique methods of learning. In aiming for more complex tasks, especially those incorporating multimodal data, one hurdle continuing to prevent their realization is an inability to orchestrate multiple networks on neuromorphic hardware without resorting to off-chip process management logic. To address this, we show a first example of a pipeline for vision-based robot control in which numerous complex networks can be run entirely on hardware via the use of a spiking neural state machine for process orchestration. The pipeline is validated on the Intel Loihi 2 research chip. We show that all components can run concurrently on-chip in the milli Watt regime at latencies competitive with the state-of-the-art. An equivalent network on simulated hardware is shown to accomplish robotic arm plug insertion in simulation, and the core elements of the pipeline are additionally tested on a real robotic arm.

### [All-Optically Controlled Memristive Reservoir Computing Capable of Bipolar and Parallel Coding](http://arxiv.org/abs/2602.12938v1)
**2026-02-13** | *Lingxiang Hu, Dian Jiao, Kexuan Wang et al.*

> Physical reservoir computing (RC) utilizes the intrinsic dynamical evolution of physical systems for efficient data processing. Emerging optoelectronic RC platforms,such as light-driven memristors, merge the benefits of electronic and photonic computation. However, conventional designs are often limited by the unipolar photoresponse of optoelectronic devices, which restricts reservoir state diversity and reduces computational accuracy. To overcome these limitations, we introduce an all-optically controlled RC system employing an oxide memristor array that demonstrates exceptional uniformity and stability. The memristive devices exhibit wavelength-dependent bipolar photoresponse, originating from light-induced dynamic evolution of oxygen vacancies. Tuning the power density and irradiation mode of dual-wavelength light pulses enables dynamic control of photocurrent relaxation and nonlinearity. By leveraging these unique device properties, we develop bipolar and parallel coding strategies to significantly enrich reservoir dynamics and enhance nonlinear mapping capability. In word recognition and time-series prediction tasks, the bipolar coding demonstrates markedly improved accuracy compared to unipolar coding. The parallel coding supports multi-source signal fusion within a single reservoir, maintaining high computational accuracy while significantly reducing hardware consumption. This work provides a high-performance approach to physical RC, paving the way for intelligent edge computing.

### [Device-Circuit Co-Design of Variation-Resilient Read and Write Drivers for Antiferromagnetic Tunnel Junction (AFMTJ) Memories](http://arxiv.org/abs/2602.11614v1)
**2026-02-12** | *Yousuf Choudhary, Tosiron Adegbija*

> Antiferromagnetic Tunnel Junctions (AFMTJs) offer picosecond switching and high integration density for in-memory computing, but their ultrafast dynamics and low tunnel magnetoresistance (TMR) make state-of-the-art MRAM interfaces unreliable. This work develops a device-circuit co-designed read/write interface optimized for AFMTJ behavior. Using a calibrated SPICE AFMTJ model as a baseline, we identify the limitations of conventional drivers and propose an asymmetric pulse driver (PD) for deterministic picosecond switching and a self-timed sense amplifier (STSA) with dynamic trip-point tuning for low-TMR sensing. Our experiments using SPICE and Monte Carlo evaluations demonstrate that the proposed circuits preserve AFMTJ latency and energy benefits while achieving robust read/write yield under realistic PVT and 3D integration parasitics, outperforming standard MRAM front-ends under the same conditions.

### [Jamming-controlled stochasticity in metal-insulator switching](http://arxiv.org/abs/2602.11302v1)
**2026-02-11** | *NicolÃ² D'Anna, Nareg Ghazikhanian, Katherine Matthews et al.*

> Understanding and controlling phase transitions is a fundamental part of physics and has been central to many technological revolutions, from steam engines to field-effect transistors. At present, there is strong interest in materials with strongly coupled structural and electronic phase transitions, which hold promise for energy-efficient technologies. Utilizing a structural phase transition and controlling its plasticity naturally leads to built-in memory, a key feature for emulating neurons and synapses in neuromorphic technologies. Here, $\textit{operando}$ Bragg X-ray photon correlation spectroscopy is used to study the evolution of the nano-domain distribution at the micron-scale in neuromorphic devices made from the archetypal Mott insulator vanadium dioxide. It is found that after electrical switching, slow nano-domain reconfiguration occurs on timescales of thousands of seconds and that the domains undergo a jamming transition, offering control over switching stochasticity at the micron scale. More precisely, repetitive above-threshold currents plastically drive the system into a jammed/glassy state where switching becomes deterministic, while sub-threshold currents erase the short-term memory contained in the nano-domain distribution, recovering stochastic switching, thus offering a path for in-device learning. The results illustrate the importance of studying the nanoscale physics associated with phase transitions in strongly correlated materials, even for macroscopic devices, and offer guidance for future device operation schemes.

### [Amortized Inference of Neuron Parameters on Analog Neuromorphic Hardware](http://arxiv.org/abs/2602.10763v2)
**2026-02-11** | *Jakob Kaiser, Eric MÃ¼ller, Johannes Schemmel*

> Our work utilized a non-sequential simulation-based inference algorithm to provide an amortized neural density estimator, which approximates the posterior distribution for seven parameters of the adaptive exponential integrate-and-fire neuron model of the analog neuromorphic BrainScaleS-2 substrate. We constrained the large parameter space by training a binary classifier to predict parameter combinations yielding observations in regimes of interest, i.e. moderate spike counts. We compared two neural density estimators: one using handcrafted summary statistics and one using a summary network trained in combination with the neural density estimator. The summary network yielded a more focused posterior and generated posterior predictive traces that accurately captured the membrane potential dynamics. When using handcrafted summary statistics, posterior predictive traces match the included features but show deviations in the exact dynamics. The posteriors showed signs of bias and miscalibration but were still able to yield posterior predictive samples that were close to the target observations on which the posteriors were constrained. Our results validate amortized simulation-based inference as a tool for parameterizing analog neuron circuits.

### [Device Applications of Heterogeneously Integrated Strain-Switched Ferrimagnets/Topological Insulator/Piezoelectric Stacks](http://arxiv.org/abs/2602.10294v1)
**2026-02-10** | *Supriyo Bandyopadhyay*

> A family of ferrimagnets (CoV2O4, GdCo, TbCo) exhibits out-of-plane magnetic anisotropy when strained compressively and in-plane magnetic anisotropy when strained expansively (or vice versa). If such a ferrimagnetic thin film is placed on top of a topological insulator (TI) thin film and its magnetic anisotropy is modulated with strain, then interfacial exchange coupling between the ferrimagnet (FM) and the underlying TI will modulate the surface current flowing through the latter. If the strain is varied continuously, the current will also vary continuously and if the strain alternates in time, the current will also alternate with the frequency of the strain modulation, as long as the frequency is not so high that the period is smaller than the switching time of the FM. If the strain is generated with a gate voltage by integrating a piezoelectric underneath the FM/TI stack, then that can implement a transconductance amplifier or a synapse for neuromorphic computation.

### [Area-Efficient In-Memory Computing for Mixture-of-Experts via Multiplexing and Caching](http://arxiv.org/abs/2602.10254v1)
**2026-02-10** | *Hanyuan Gao, Xiaoxuan Yang*

> Mixture-of-Experts (MoE) layers activate a subset of model weights, dubbed experts, to improve model performance. MoE is particularly promising for deployment on process-in-memory (PIM) architectures, because PIM can naturally fit experts separately and provide great benefits for energy efficiency. However, PIM chips often suffer from large area overhead, especially in the peripheral circuits. In this paper, we propose an area-efficient in-memory computing architecture for MoE transformers. First, to reduce area, we propose a crossbar-level multiplexing strategy that exploits MoE sparsity: experts are deployed on crossbars and multiple crossbars share the same peripheral circuits. Second, we propose expert grouping and group-wise scheduling methods to alleviate the load imbalance and contention overhead caused by sharing. In addition, to address the problem that the expert choice router requires access to all hidden states during generation, we propose a gate-output (GO)cache to store necessary results and bypass expensive additional computation. Experiments show that our approaches improve the area efficiency of the MoE part by up to 2.2x compared to a SOTA architecture. During generation, the cache improves performance and energy efficiency by 4.2x and 10.1x, respectively, compared to the baseline when generating 8 tokens. The total performance density achieves 15.6 GOPS/W/mm2. The code is open source at https://github.com/superstarghy/MoEwithPIM.

### [Efficient Deep Learning for Biometrics: Overview, Challenges and Trends in Ear of Frugal AI](http://arxiv.org/abs/2602.08809v1)
**2026-02-09** | *Karim Haroun, Aya Zitouni, Aicha Zenakhri et al.*

> Recent advances in deep learning, whether on discriminative or generative tasks have been beneficial for various applications, among which security and defense. However, their increasing computational demands during training and deployment translates directly into high energy consumption. As a consequence, this induces a heavy carbon footprint which hinders their widespread use and scalability, but also a limitation when deployed on resource-constrained edge devices for real-time use. In this paper, we briefly survey efficient deep learning methods for biometric applications. Specifically, we tackle the challenges one might incur when training and deploying deep learning approaches, and provide a taxonomy of the various efficient deep learning families. Additionally, we discuss complementary metrics for evaluating the efficiency of these models such as memory, computation, latency, throughput, and advocate for universal and reproducible metrics for better comparison. Last, we give future research directions to consider.

### [2D ferroelectric narrow-bandgap semiconductor Wurtzite' type alpha-In2Se3 and its silicon-compatible growth](http://arxiv.org/abs/2602.08381v1)
**2026-02-09** | *Yuxuan Jiang, Xingkun Ning, Renhui Liu et al.*

> 2D van der Waals ferroelectrics, particularly alpha-In2Se3, have emerged as an attractive building block for next-generation information storage technologies due to their moderate band gap and robust ferroelectricity stabilized by dipole locking. alpha-In2Se3 can adopt either the distorted zincblende or wurtzite structures; however, the wurtzite phase has yet to be experimental-ly validated, and its large-scale synthesis poses significant challenges. Here, we report an in-situ transport growth of centimeter-scale wurtzite type alpha-In2Se3 films directly on SiO2 substrates using a process combining pulsed laser deposition and chemical vapor deposition. We demonstrate that it is a narrow bandgap ferroelectric semiconductor, featuring a Curie tem-perature exceeding 620 K, a tunable bandgap (0.8-1.6 eV) modulated by charged domain walls, and a large optical absorption coefficient of 1.3 times 10 powers 6 per centemeter. Moreover, light absorption promotes the dynamic conductance range, linearity, and symmetry of the synapse devices, leading to a high recognition accuracy of 92.3 percent in a supervised pattern classification task for neuromorphic computing. Our findings demonstrate a ferroelectric polymorphism of In2Se3, highlighting its potential in ferroelectric synapses for neuromorphic computing.

### [Antiferromagnetic Tunnel Junctions (AFMTJs) for In-Memory Computing: Modeling and Case Study](http://arxiv.org/abs/2602.08323v1)
**2026-02-09** | *Yousuf Choudhary, Tosiron Adegbija*

> Antiferromagnetic Tunnel Junctions (AFMTJs) enable picosecond switching and femtojoule writes through ultrafast sublattice dynamics. We present the first end-to-end AFMTJ simulation framework integrating multi-sublattice Landau-Lifshitz-Gilbert (LLG) dynamics with circuit-level modeling. SPICE-based simulations show that AFMTJs achieve ~8x lower write latency and ~9x lower write energy than conventional MTJs. When integrated into an in-memory computing architecture, AFMTJs deliver 17.5x average speedup and nearly 20x energy savings versus a CPU baseline-significantly outperforming MTJ-based IMC. These results establish AFMTJs as a compelling primitive for scalable, low-power computing.

### [Photonic neuromorphic processing with coupled spiking silicon microrings](http://arxiv.org/abs/2602.05918v1)
**2026-02-05** | *Giovanni Donati, Stefano Biasi, Lorenzo Pavesi et al.*

> Understanding the physical computing mechanisms of individual network nodes is essential for scaling neuromorphic photonic architectures. This work proposes a compact passive nonlinear photonic core based on a Side-Coupled Integrated Spaced Sequence of Resonators (SCISSOR) made of three nominally equal microrings and investigate its computing capabilities. Its nonlinearities and internal feedback enable analogue, spiking, and bistable responses that are accessed by tuning the injection power and wavelength. Implemented as a single nonlinear node in a time-multiplexed reservoir computing, the SCISSOR achieves error-free classification on the Iris dataset and accuracies above 97% on the Sonar task, using both analogue and digital reservoir representations with 150 virtual nodes. In the digital scheme, spiking dynamics naturally generate sparse reservoir states, enabling efficient classification even with a single spike. Intriguingly, optimal operating points are at the boundaries where sharp transitions in dynamical complexity and/or output power occur. In these points, the SCISSOR supports high task-performance, opening novel strategies for future on-chip training. Spiking and thermal bistabilities also participate to enhance the computational performance at low injected powers below 4 mW. These results suggest optical coupled microring resonators as effective building blocks for future edge computing and neuromorphic photonic systems.

---

## ðŸ§  Algorithms & Theory

### [Unbiased Gradient Estimation for Event Binning via Functional Backpropagation](http://arxiv.org/abs/2602.12590v1)
**2026-02-13** | *Jinze Chen, Wei Zhai, Han Han et al.*

> Event-based vision encodes dynamic scenes as asynchronous spatio-temporal spikes called events. To leverage conventional image processing pipelines, events are typically binned into frames. However, binning functions are discontinuous, which truncates gradients at the frame level and forces most event-based algorithms to rely solely on frame-based features. Attempts to directly learn from raw events avoid this restriction but instead suffer from biased gradient estimation due to the discontinuities of the binning operation, ultimately limiting their learning efficiency. To address this challenge, we propose a novel framework for unbiased gradient estimation of arbitrary binning functions by synthesizing weak derivatives during backpropagation while keeping the forward output unchanged. The key idea is to exploit integration by parts: lifting the target functions to functionals yields an integral form of the derivative of the binning function during backpropagation, where the cotangent function naturally arises. By reconstructing this cotangent function from the sampled cotangent vector, we compute weak derivatives that provably match long-range finite differences of both smooth and non-smooth targets. Experimentally, our method improves simple optimization-based egomotion estimation with 3.2\% lower RMS error and 1.57$\times$ faster convergence. On complex downstream tasks, we achieve 9.4\% lower EPE in self-supervised optical flow, and 5.1\% lower RMS error in SLAM, demonstrating broad benefits for event-based visual perception. Source code can be found at https://github.com/chjz1024/EventFBP.

### [On the Sensitivity of Firing Rate-Based Federated Spiking Neural Networks to Differential Privacy](http://arxiv.org/abs/2602.12009v1)
**2026-02-12** | *Luiz Pereira, Mirko Perkusich, Dalton Valadares et al.*

> Federated Neuromorphic Learning (FNL) enables energy-efficient and privacy-preserving learning on devices without centralizing data. However, real-world deployments require additional privacy mechanisms that can significantly alter training signals. This paper analyzes how Differential Privacy (DP) mechanisms, specifically gradient clipping and noise injection, perturb firing-rate statistics in Spiking Neural Networks (SNNs) and how these perturbations are propagated to rate-based FNL coordination. On a speech recognition task under non-IID settings, ablations across privacy budgets and clipping bounds reveal systematic rate shifts, attenuated aggregation, and ranking instability during client selection. Moreover, we relate these shifts to sparsity and memory indicators. Our findings provide actionable guidance for privacy-preserving FNL, specifically regarding the balance between privacy strength and rate-dependent coordination.

### [Information Abstraction for Data Transmission Networks based on Large Language Models](http://arxiv.org/abs/2602.11022v1)
**2026-02-11** | *Haoyuan Zhu, Haonan Hu, Jie Zhang*

> Biological systems, particularly the human brain, achieve remarkable energy efficiency by abstracting information across multiple hierarchical levels. In contrast, modern artificial intelligence and communication systems often consume significant energy overheads in transmitting low-level data, with limited emphasis on abstraction. Despite its implicit importance, a formal and computational theory of information abstraction remains absent. In this work, we introduce the Degree of Information Abstraction (DIA), a general metric that quantifies how well a representation compresses input data while preserving task-relevant semantics. We derive a tractable information-theoretic formulation of DIA and propose a DIA-based information abstraction framework. As a case study, we apply DIA to a large language model (LLM)-guided video transmission task, where abstraction-aware encoding significantly reduces transmission volume by $99.75\%$, while maintaining semantic fidelity. Our results suggest that DIA offers a principled tool for rebalancing energy and information in intelligent systems and opens new directions in neural network design, neuromorphic computing, semantic communication, and joint sensing-communication architectures.

### [Nonlinear dynamics in magnonic Fabry-PÃ©rot resonators: Low-power neuron-like activation and transmission suppression](http://arxiv.org/abs/2602.10650v1)
**2026-02-11** | *Anton Lutsenko, Kevin G. Fripp, LukÃ¡Å¡ FlajÅ¡man et al.*

> We report on nonlinear spin-wave dynamics in magnonic Fabry-PÃ©rot resonators composed of yttrium iron garnet (YIG) films coupled to CoFeB nanostripes. Using super-Nyquist sampling magneto-optical Kerr effect microscopy and micromagnetic simulations, we observe a systematic downshift of the spin-wave transmission gaps as the excitation power increases. This nonlinear behavior occurs at low power levels, reduced by a strong spatial concentration of spin waves within the resonator. The resulting power-dependent transmission enables neuron-like activation behavior and frequency-selective nonlinear spin-wave absorption. Our results highlight magnonic Fabry-PÃ©rot resonators as compact low-power nonlinear elements for neuromorphic magnonic computing architectures.

### [UltraLIF: Fully Differentiable Spiking Neural Networks via Ultradiscretization and Max-Plus Algebra](http://arxiv.org/abs/2602.11206v1)
**2026-02-10** | *Jose Marie Antonio MiÃ±oza*

> Spiking Neural Networks (SNNs) offer energy-efficient, biologically plausible computation but suffer from non-differentiable spike generation, necessitating reliance on heuristic surrogate gradients. This paper introduces UltraLIF, a principled framework that replaces surrogate gradients with ultradiscretization, a mathematical formalism from tropical geometry providing continuous relaxations of discrete dynamics. The central insight is that the max-plus semiring underlying ultradiscretization naturally models neural threshold dynamics: the log-sum-exp function serves as a differentiable soft-maximum that converges to hard thresholding as a learnable temperature parameter $\eps \to 0$. Two neuron models are derived from distinct dynamical systems: UltraLIF from the LIF ordinary differential equation (temporal dynamics) and UltraDLIF from the diffusion equation modeling gap junction coupling across neuronal populations (spatial dynamics). Both yield fully differentiable SNNs trainable via standard backpropagation with no forward-backward mismatch. Theoretical analysis establishes pointwise convergence to classical LIF dynamics with quantitative error bounds and bounded non-vanishing gradients. Experiments on six benchmarks spanning static images, neuromorphic vision, and audio demonstrate improvements over surrogate gradient baselines, with gains most pronounced in single-timestep ($T{=}1$) settings on neuromorphic and temporal datasets. An optional sparsity penalty enables significant energy reduction while maintaining competitive accuracy.

### [Sparse Axonal and Dendritic Delays Enable Competitive SNNs for Keyword Classification](http://arxiv.org/abs/2602.09746v1)
**2026-02-10** | *Younes Bouhadjar, Emre Neftci*

> Training transmission delays in spiking neural networks (SNNs) has been shown to substantially improve their performance on complex temporal tasks. In this work, we show that learning either axonal or dendritic delays enables deep feedforward SNNs composed of leaky integrate-and-fire (LIF) neurons to reach accuracy comparable to existing synaptic delay learning approaches, while significantly reducing memory and computational overhead. SNN models with either axonal or dendritic delays achieve up to $95.58\%$ on the Google Speech Command (GSC) and $80.97\%$ on the Spiking Speech Command (SSC) datasets, matching or exceeding prior methods based on synaptic delays or more complex neuron models. By adjusting the delay parameters, we obtain improved performance for synaptic delay learning baselines, strengthening the comparison. We find that axonal delays offer the most favorable trade-off, combining lower buffering requirements with slightly higher accuracy than dendritic delays. We further show that the performance of axonal and dendritic delay models is largely preserved under strong delay sparsity, with as few as $20\%$ of delays remaining active, further reducing buffering requirements. Overall, our results indicate that learnable axonal and dendritic delays provide a resource-efficient and effective mechanism for temporal representation in SNNs. Code will be made available publicly upon acceptance. Code is available at https://github.com/YounesBouhadjar/AxDenSynDelaySNN

### [From Lightweight CNNs to SpikeNets: Benchmarking Accuracy-Energy Tradeoffs with Pruned Spiking SqueezeNet](http://arxiv.org/abs/2602.09717v1)
**2026-02-10** | *Radib Bin Kabir, Tawsif Tashwar Dipto, Mehedi Ahamed et al.*

> Spiking Neural Networks (SNNs) are increasingly studied as energy-efficient alternatives to Convolutional Neural Networks (CNNs), particularly for edge intelligence. However, prior work has largely emphasized large-scale models, leaving the design and evaluation of lightweight CNN-to-SNN pipelines underexplored. In this paper, we present the first systematic benchmark of lightweight SNNs obtained by converting compact CNN architectures into spiking networks, where activations are modeled with Leaky-Integrate-and-Fire (LIF) neurons and trained using surrogate gradient descent under a unified setup. We construct spiking variants of ShuffleNet, SqueezeNet, MnasNet, and MixNet, and evaluate them on CIFAR-10, CIFAR-100, and TinyImageNet, measuring accuracy, F1-score, parameter count, computational complexity, and energy consumption. Our results show that SNNs can achieve up to 15.7x higher energy efficiency than their CNN counterparts while retaining competitive accuracy. Among these, the SNN variant of SqueezeNet consistently outperforms other lightweight SNNs. To further optimize this model, we apply a structured pruning strategy that removes entire redundant modules, yielding a pruned architecture, SNN-SqueezeNet-P. This pruned model improves CIFAR-10 accuracy by 6% and reduces parameters by 19% compared to the original SNN-SqueezeNet. Crucially, it narrows the gap with CNN-SqueezeNet, achieving nearly the same accuracy (only 1% lower) but with an 88.1% reduction in energy consumption due to sparse spike-driven computations. Together, these findings establish lightweight SNNs as practical, low-power alternatives for edge deployment, highlighting a viable path toward deploying high-performance, low-power intelligence on the edge.

### [Kirin: Improving ANN efficiency with SNN Hybridization](http://arxiv.org/abs/2602.08817v1)
**2026-02-09** | *Chenyu Wang, Zhanglu Yan, Zhi Zhou et al.*

> Artificial neural networks (ANNs), particularly large language models (LLMs), demonstrate powerful inference capabilities but consume substantial energy. Conversely, spiking neural networks (SNNs) exhibit exceptional energy efficiency due to their binary and event-driven characteristics, thus motivating the study of ANN-to-SNN conversion. In this process, quantization plays a pivotal role, mapping LLMs' floating-point parameters to discrete SNN parameters via the temporal dimension of the time window. However, several challenges remain in the conversion process: (i) converting high bit-width quantization values into binary spikes requires longer time windows, increasing system latency; and (ii) the inherent trade-off between the information loss of single-spike schemes and the energy costs of multi-spike ones in SNN. To address these challenges, we propose Kirin, a integer and spike hybrid based SNN to achieve accuracy lossless ANN-to-SNN conversion with time and energy efficiency. Specifically, we first propose a Spike Matrix Hybridization strategy that encoding low bit-width parameters that leading to small time window size into binary spikes while preserving the rest in integer format, thereby reducing the overall latency of SNN execution. Second, we introduce a silence threshold mechanism to regulate the timing of single-spike firing, ensuring the output is mathematically equivalent to the LLM's output and preserves accuracy. Experimental results demonstrate that Kirin, under a W4A4\&8 quantization setting, achieves near-FP16 accuracy while reducing energy consumption by up to 84.66\% and shortening time steps by 93.75\%.

### [PTS-SNN: A Prompt-Tuned Temporal Shift Spiking Neural Networks for Efficient Speech Emotion Recognition](http://arxiv.org/abs/2602.08240v1)
**2026-02-09** | *Xun Su, Huamin Wang, Qi Zhang*

> Speech Emotion Recognition (SER) is widely deployed in Human-Computer Interaction, yet the high computational cost of conventional models hinders their implementation on resource-constrained edge devices. Spiking Neural Networks (SNNs) offer an energy-efficient alternative due to their event-driven nature; however, their integration with continuous Self-Supervised Learning (SSL) representations is fundamentally challenged by distribution mismatch, where high-dynamic-range embeddings degrade the information coding capacity of threshold-based neurons. To resolve this, we propose Prompt-Tuned Spiking Neural Networks (PTS-SNN), a parameter-efficient neuromorphic adaptation framework that aligns frozen SSL backbones with spiking dynamics. Specifically, we introduce a Temporal Shift Spiking Encoder to capture local temporal dependencies via parameter-free channel shifts, establishing a stable feature basis. To bridge the domain gap, we devise a Context-Aware Membrane Potential Calibration strategy. This mechanism leverages a Spiking Sparse Linear Attention module to aggregate global semantic context into learnable soft prompts, which dynamically regulate the bias voltages of Parametric Leaky Integrate-and-Fire (PLIF) neurons. This regulation effectively centers the heterogeneous input distribution within the responsive firing range, mitigating functional silence or saturation. Extensive experiments on five multilingual datasets (e.g., IEMOCAP, CASIA, EMODB) demonstrate that PTS-SNN achieves 73.34\% accuracy on IEMOCAP, comparable to competitive Artificial Neural Networks (ANNs), while requiring only 1.19M trainable parameters and 0.35 mJ inference energy per sample.

### [BadSNN: Backdoor Attacks on Spiking Neural Networks via Adversarial Spiking Neuron](http://arxiv.org/abs/2602.07200v1)
**2026-02-06** | *Abdullah Arafat Miah, Kevin Vu, Yu Bi*

> Spiking Neural Networks (SNNs) are energy-efficient counterparts of Deep Neural Networks (DNNs) with high biological plausibility, as information is transmitted through temporal spiking patterns. The core element of an SNN is the spiking neuron, which converts input data into spikes following the Leaky Integrate-and-Fire (LIF) neuron model. This model includes several important hyperparameters, such as the membrane potential threshold and membrane time constant. Both the DNNs and SNNs have proven to be exploitable by backdoor attacks, where an adversary can poison the training dataset with malicious triggers and force the model to behave in an attacker-defined manner. Yet, how an adversary can exploit the unique characteristics of SNNs for backdoor attacks remains underexplored. In this paper, we propose \textit{BadSNN}, a novel backdoor attack on spiking neural networks that exploits hyperparameter variations of spiking neurons to inject backdoor behavior into the model. We further propose a trigger optimization process to achieve better attack performance while making trigger patterns less perceptible. \textit{BadSNN} demonstrates superior attack performance on various datasets and architectures, as well as compared with state-of-the-art data poisoning-based backdoor attacks and robustness against common backdoor mitigation techniques. Codes can be found at https://github.com/SiSL-URI/BadSNN.

### [Sparse Spike Encoding of Channel Responses for Energy Efficient Human Activity Recognition](http://arxiv.org/abs/2602.06766v1)
**2026-02-06** | *Eleonora Cicciarella, Riccardo Mazzieri, Jacopo Pegoraro et al.*

> ISAC enables pervasive monitoring, but modern sensing algorithms are often too complex for energy-constrained edge devices. This motivates the development of learning techniques that balance accuracy performance and energy efficiency. Spiking Neural Networks (SNNs) are a promising alternative, processing information as sparse binary spike trains and potentially reducing energy consumption by orders of magnitude. In this work, we propose a spiking convolutional autoencoder (SCAE) that learns tailored spike-encoded representations of channel impulse responses (CIR), jointly trained with an SNN for human activity recognition (HAR), thereby eliminating the need for Doppler domain preprocessing. The results show that our SCAE-SNN achieves F1 scores comparable to a hybrid approach (almost 96%), while producing substantially sparser spike encoding (81.1% sparsity). We also show that encoding CIR data prior to classification improves both HAR accuracy and efficiency. The code is available at https://github.com/ele-ciccia/SCAE-SNN-HAR.

---

## ðŸ‘ï¸ Applications & Sensing

### [Neurosim: A Fast Simulator for Neuromorphic Robot Perception](http://arxiv.org/abs/2602.15018v1)
**2026-02-16** | *Richeek Das, Pratik Chaudhari*

> Neurosim is a fast, real-time, high-performance library for simulating sensors such as dynamic vision sensors, RGB cameras, depth sensors, and inertial sensors. It can also simulate agile dynamics of multi-rotor vehicles in complex and dynamic environments. Neurosim can achieve frame rates as high as ~2700 FPS on a desktop GPU. Neurosim integrates with a ZeroMQ-based communication library called Cortex to facilitate seamless integration with machine learning and robotics workflows. Cortex provides a high-throughput, low-latency message-passing system for Python and C++ applications, with native support for NumPy arrays and PyTorch tensors. This paper discusses the design philosophy behind Neurosim and Cortex. It demonstrates how they can be used to (i) train neuromorphic perception and control algorithms, e.g., using self-supervised learning on time-synchronized multi-modal data, and (ii) test real-time implementations of these algorithms in closed-loop. Neurosim and Cortex are available at https://github.com/grasp-lyrl/neurosim .

### [Energy-Aware Spike Budgeting for Continual Learning in Spiking Neural Networks for Neuromorphic Vision](http://arxiv.org/abs/2602.12236v1)
**2026-02-12** | *Anika Tabassum Meem, Muntasir Hossain Nadid, Md Zesun Ahmed Mia*

> Neuromorphic vision systems based on spiking neural networks (SNNs) offer ultra-low-power perception for event-based and frame-based cameras, yet catastrophic forgetting remains a critical barrier to deployment in continually evolving environments. Existing continual learning methods, developed primarily for artificial neural networks, seldom jointly optimize accuracy and energy efficiency, with particularly limited exploration on event-based datasets. We propose an energy-aware spike budgeting framework for continual SNN learning that integrates experience replay, learnable leaky integrate-and-fire neuron parameters, and an adaptive spike scheduler to enforce dataset-specific energy constraints during training. Our approach exhibits modality-dependent behavior: on frame-based datasets (MNIST, CIFAR-10), spike budgeting acts as a sparsity-inducing regularizer, improving accuracy while reducing spike rates by up to 47\%; on event-based datasets (DVS-Gesture, N-MNIST, CIFAR-10-DVS), controlled budget relaxation enables accuracy gains up to 17.45 percentage points with minimal computational overhead. Across five benchmarks spanning both modalities, our method demonstrates consistent performance improvements while minimizing dynamic power consumption, advancing the practical viability of continual learning in neuromorphic vision systems.

### [SynSacc: A Blender-to-V2E Pipeline for Synthetic Neuromorphic Eye-Movement Data and Sim-to-Real Spiking Model Training](http://arxiv.org/abs/2602.08726v1)
**2026-02-09** | *Khadija Iddrisu, Waseem Shariff, Suzanne Little et al.*

> The study of eye movements, particularly saccades and fixations, are fundamental to understanding the mechanisms of human cognition and perception. Accurate classification of these movements requires sensing technologies capable of capturing rapid dynamics without distortion. Event cameras, also known as Dynamic Vision Sensors (DVS), provide asynchronous recordings of changes in light intensity, thereby eliminating motion blur inherent in conventional frame-based cameras and offering superior temporal resolution and data efficiency. In this study, we introduce a synthetic dataset generated with Blender to simulate saccades and fixations under controlled conditions. Leveraging Spiking Neural Networks (SNNs), we evaluate its robustness by training two architectures and finetuning on real event data. The proposed models achieve up to 0.83 accuracy and maintain consistent performance across varying temporal resolutions, demonstrating stability in eye movement classification. Moreover, the use of SNNs with synthetic event streams yields substantial computational efficiency gains over artificial neural network (ANN) counterparts, underscoring the utility of synthetic data augmentation in advancing event-based vision. All code and datasets associated with this work is available at https: //github.com/Ikhadija-5/SynSacc-Dataset.

### [A neuromorphic model of the insect visual system for natural image processing](http://arxiv.org/abs/2602.06405v1)
**2026-02-06** | *Adam D. Hines, Karin NordstrÃ¶m, Andrew B. Barron*

> Insect vision supports complex behaviors including associative learning, navigation, and object detection, and has long motivated computational models for understanding biological visual processing. However, many contemporary models prioritize task performance while neglecting biologically grounded processing pathways. Here, we introduce a bio-inspired vision model that captures principles of the insect visual system to transform dense visual input into sparse, discriminative codes. The model is trained using a fully self-supervised contrastive objective, enabling representation learning without labeled data and supporting reuse across tasks without reliance on domain-specific classifiers. We evaluated the resulting representations on flower recognition tasks and natural image benchmarks. The model consistently produced reliable sparse codes that distinguish visually similar inputs. To support different modelling and deployment uses, we have implemented the model as both an artificial neural network and a spiking neural network. In a simulated localization setting, our approach outperformed a simple image downsampling comparison baseline, highlighting the functional benefit of incorporating neuromorphic visual processing pathways. Collectively, these results advance insect computational modelling by providing a generalizable bio-inspired vision model capable of sparse computation across diverse tasks.

---

