# ðŸ§  Open Neuromorphic - Daily ArXiv

**Automated Daily Update** | Last Run: 2026-01-21 08:31 UTC

Papers are automatically categorized by topic and sorted by date.

## ðŸ›  Hardware & Materials

### [Polychronous Wave Computing: Timing-Native Address Selection in Spiking Networks](http://arxiv.org/abs/2601.13079v1)
**2026-01-19** | *Natalila G. Berloff*

> Spike timing offers a combinatorial address space, suggesting that timing-based spiking inference can be executed as lookup and routing rather than as dense multiply--accumulate. Yet most neuromorphic and photonic systems still digitize events into timestamps, bins, or rates and then perform selection in clocked logic. We introduce Polychronous Wave Computing (PWC), a timing-native address-selection primitive that maps relative spike latencies directly to a discrete output route in the wave domain. Spike times are phase-encoded in a rotating frame and processed by a programmable multiport interferometer that evaluates K template correlations in parallel; a driven--dissipative winner-take-all stage then performs a physical argmax, emitting a one-hot output port. We derive the operating envelope imposed by phase wrapping and mutual coherence, and collapse timing jitter, static phase mismatch, and dephasing into a single effective phase-noise budget whose induced winner--runner-up margin predicts boundary-first failures and provides an intensity-only calibration target. Simulations show that nonlinear competition improves routing fidelity compared with noisy linear intensity readout, and that hardware-in-the-loop phase tuning rescues a temporal-order gate from 55.9% to 97.2% accuracy under strong static mismatch. PWC provides a fast routing coprocessor for LUT-style spiking networks and sparse top-1 gates (e.g., mixture-of-experts routing) across polaritonic, photonic, and oscillator platforms.

### [Biological Intuition on Digital Hardware: An RTL Implementation of Poisson-Encoded SNNs for Static Image Classification](http://arxiv.org/abs/2601.12156v1)
**2026-01-17** | *Debabrata Das, Yogeeth G. K., Arnav Gupta*

> The deployment of Artificial Intelligence on edge devices (TinyML) is often constrained by the high power consumption and latency associated with traditional Artificial Neural Networks (ANNs) and their reliance on intensive Matrix-Multiply (MAC) operations. Neuromorphic computing offers a compelling alternative by mimicking biological efficiency through event-driven processing. This paper presents the design and implementation of a cycle-accurate, hardware-oriented Spiking Neural Network (SNN) core implemented in SystemVerilog. Unlike conventional accelerators, this design utilizes a Leaky Integrate-and-Fire (LIF) neuron model powered by fixed-point arithmetic and bit-wise primitives (shifts and additions) to eliminate the need for complex floating-point hardware. The architecture features an on-chip Poisson encoder for stochastic spike generation and a novel active pruning mechanism that dynamically disables neurons post-classification to minimize dynamic power consumption. We demonstrate the hardware's efficacy through a fully connected layer implementation targeting digit classification. Simulation results indicate that the design achieves rapid convergence (89% accuracy) within limited timesteps while maintaining a significantly reduced computational footprint compared to traditional dense architectures. This work serves as a foundational building block for scalable, energy-efficient neuromorphic hardware on FPGA and ASIC platforms.

### [Integrated nano electro-optomechanical spiking neuron](http://arxiv.org/abs/2601.11857v1)
**2026-01-17** | *Gregorio Beltramo, RÃ³bert HorvÃ¡th, GrÃ©goire Beaudoin et al.*

> Neuromorphic computing offers a pathway toward energy-efficient processing of data, yet hardware platforms combining nanoscale integration and multimodal functionality remain scarce. Here we demonstrate a gallium-phosphide electro-optomechanical spiking neuron that integrates optical and electromechanical interfaces within a single nanostructure on a silicon photonic chip operating at telecommunication wavelengths (1550 nm) and exploiting a 3 gigahertz-frequency mechanical mode. Our device displays excitable dynamics, generating optical spikes at its output, as in the spiking activity of neurons and cardiac cells and defined by the calibrated all-or-none response to external perturbations. This dynamic is consistent with the saddle-node on invariant circle scenario and associated features are demonstrated including control of excitable threshold, temporal summation and refractory period. Our device compact footprint and its CMOS-compatible platform make it well suited for edge-computing applications requiring low latency and establish a foundation for versatile brain-inspired optomechanical computing and advanced on-chip optical pulse sources.

### [Effects of Integrated Heatsinking on Superconductivity in Tantalum Nitride Nanowires at the 300 Millimeter Scale](http://arxiv.org/abs/2601.10480v1)
**2026-01-15** | *Ekta Bhatia, Tharanga R. Nanayakkara, Chenyu Zhou et al.*

> We report the superconducting properties of tantalum nitride (TaN) nanowires and TaN/copper (TaN/Cu) bilayer nanowires fabricated on 300 mm silicon wafers using CMOS-compatible processes. We evaluate how an integrated Cu heatsink modifies the superconducting response of TaN nanowires by improving thermal dissipation without significantly compromising key superconducting parameters. Through analysis of hysteresis in current-voltage curves, we demonstrate that Cu integration improves heat dissipation, supporting expectations of faster reset times in superconducting nanowire single-photon detectors (SNSPDs), consistent with enhanced heat transfer away from the hot spot. Using the Skocpol-Beasley-Tinkham (SBT) hotspot model, we quantify the Cu-enabled improvement in heat transfer as an approximately 100x increase in the SBT slope parameter beta and effective interfacial heat-transfer efficiency compared to TaN nanowires. The near-unity ratio of critical to retrapping current in TaN/Cu bilayer nanowires provides another evidence of efficient heat removal enabled by the integrated Cu layer. Our results show a zero-temperature Ginzburg-Landau coherence length of 7 nm and a critical temperature of 4.1 K for 39 nm thick TaN nanowires. The nanowires show <5% variation in critical dimensions, room-temperature resistance, residual resistance ratio, critical temperature, and critical current across the 300 mm wafer for all measured linewidths, demonstrating excellent process uniformity and scalability. These results indicate the trade-offs between superconducting performance and heat-sinking efficiency in TaN/Cu bilayer nanowires. They also underscore the viability of wafer-scale fabrication for fast, large-area SNSPD arrays for applications in photonic quantum computing, cosmology, and neuromorphic computing devices.

### [Bridging Superconducting and Neutral-Atom Platforms for Efficient Fault-Tolerant Quantum Architectures](http://arxiv.org/abs/2601.10144v1)
**2026-01-15** | *Xiang Fang, Jixuan Ruan, Sharanya Prabhu et al.*

> The transition to the fault-tolerant era exposes the limitations of homogeneous quantum systems, where no single qubit modality simultaneously offers optimal operation speed, connectivity, and scalability. In this work, we propose a strategic approach to Heterogeneous Quantum Architectures (HQA) that synthesizes the distinct advantages of the superconducting (SC) and neutral atom (NA) platforms. We explore two architectural role assignment strategies based on hardware characteristics: (1) We offload the latency-critical Magic State Factory (MSF) to fast SC devices while performing computation on scalable NA arrays, a design we term MagicAcc, which effectively mitigates the resource-preparation bottleneck. (2) We explore a Memory-Compute Separation (MCSep) paradigm that utilizes NA arrays for high-density qLDPC memory storage and SC devices for fast surface-code processing. Our evaluation, based on a comprehensive end-to-end cost model, demonstrates that principled heterogeneity yields significant performance gains. Specifically, our designs achieve $752\times$ speedup over NA-only baselines on average and reduce the physical qubit footprint by over $10\times$ compared to SC-only systems. These results chart a clear pathway for leveraging cross-modality interconnects to optimize the space-time efficiency of future fault-tolerant quantum computers.

### [Resistive Memory based Efficient Machine Unlearning and Continual Learning](http://arxiv.org/abs/2601.10037v1)
**2026-01-15** | *Ning Lin, Jichang Yang, Yangu He et al.*

> Resistive memory (RM) based neuromorphic systems can emulate synaptic plasticity and thus support continual learning, but they generally lack biologically inspired mechanisms for active forgetting, which are critical for meeting modern data privacy requirements. Algorithmic forgetting, or machine unlearning, seeks to remove the influence of specific data from trained models to prevent memorization of sensitive information and the generation of harmful content, yet existing exact and approximate unlearning schemes incur prohibitive programming overheads on RM hardware owing to device variability and iterative write-verify cycles. Analogue implementations of continual learning face similar barriers. Here we present a hardware-software co-design that enables an efficient training, deployment and inference pipeline for machine unlearning and continual learning on RM accelerators. At the software level, we introduce a low-rank adaptation (LoRA) framework that confines updates to compact parameter branches, substantially reducing the number of trainable parameters and therefore the training cost. At the hardware level, we develop a hybrid analogue-digital compute-in-memory system in which well-trained weights are stored in analogue RM arrays, whereas dynamic LoRA updates are implemented in a digital computing unit with SRAM buffer. This hybrid architecture avoids costly reprogramming of analogue weights and maintains high energy efficiency during inference. Fabricated in a 180 nm CMOS process, the prototype achieves up to a 147.76-fold reduction in training cost, a 387.95-fold reduction in deployment overhead and a 48.44-fold reduction in inference energy across privacy-sensitive tasks including face recognition, speaker authentication and stylized image generation, paving the way for secure and efficient neuromorphic intelligence at the edge.

### [A Compute and Communication Runtime Model for Loihi 2](http://arxiv.org/abs/2601.10035v1)
**2026-01-15** | *Jonathan Timcheck, Alessandro Pierro, Sumit Bam Shrestha*

> Neuromorphic computers hold the potential to vastly improve the speed and efficiency of a wide range of computational kernels with their asynchronous, compute-memory co-located, spatially distributed, and scalable nature. However, performance models that are simple yet sufficiently expressive to predict runtime on actual neuromorphic hardware are lacking, posing a challenge for researchers and developers who strive to design fast algorithms and kernels. As breaking the memory bandwidth wall of conventional von-Neumann architectures is a primary neuromorphic advantage, modeling communication time is especially important. At the same time, modeling communication time is difficult, as complex congestion patterns arise in a heavily-loaded Network-on-Chip. In this work, we introduce the first max-affine lower-bound runtime model -- a multi-dimensional roofline model -- for Intel's Loihi 2 neuromorphic chip that quantitatively accounts for both compute and communication based on a suite of microbenchmarks. Despite being a lower-bound model, we observe a tight correspondence (Pearson correlation coefficient greater than or equal to 0.97) between our model's estimated runtime and the measured runtime on Loihi 2 for a neural network linear layer, i.e., matrix-vector multiplication, and for an example application, a Quadratic Unconstrained Binary Optimization solver. Furthermore, we derive analytical expressions for communication-bottlenecked runtime to study scalability of the linear layer, revealing an area-runtime tradeoff for different spatial workload configurations with linear to superliner runtime scaling in layer size with a variety of constant factors. Our max-affine runtime model helps empower the design of high-speed algorithms and kernels for Loihi 2.

### [Forward-only learning in memristor arrays with month-scale stability](http://arxiv.org/abs/2601.09903v1)
**2026-01-14** | *Adrien Renaudineau, Mamadou Hawa Diallo, ThÃ©o Dupuis et al.*

> Turning memristor arrays from efficient inference engines into systems capable of on-chip learning has proved difficult. Weight updates have a high energy cost and cause device wear, analog states drift, and backpropagation requires a backward pass with reversed signal flow. Here we experimentally demonstrate learning on standard filamentary HfOx/Ti arrays that addresses these challenges with two design choices. First, we realize that standard filamentary HfOx/Ti memristors support sub-1 V reset-only pulses that cut energy, improve endurance, and yield stable analog states. Second, we rely on forward-only training algorithms derived from Hinton's Forward-Forward that use only inference-style operations. We train two-layer classifiers on an ImageNet-resolution four-class task using arrays up to 8,064 devices. Two forward-only variants, the double-pass supervised Forward-Forward and a single-pass competitive rule, achieve test accuracies of 89.5% and 89.6%, respectively; a reference experiment using backpropagation reaches 90.0%. Across five independent runs per method, these accuracies match within statistical uncertainty. Trained models retain accuracy for at least one month under ambient conditions, consistent with the stability of reset-only states. Sub-1 V reset updates use 460 times less energy than conventional program-and-verify programming and require just 46% more energy than inference-only operation. Together, these results establish forward-only, sub-1 V learning on standard filamentary stacks at array scale, outlining a practical, pulse-aware route to adaptive edge intelligence.

### [Criticality in memristor devices and the creation of deep memory](http://arxiv.org/abs/2601.09464v1)
**2026-01-14** | *Stavros G. Stavrinides, Yiannis Contoyiannis*

> In the present work we describe a way to assess memory capability of real devices, while proposing to the engineering community what to pursue to create devices with deep associated memory capability. The study of the signal produced by a real memristor nano-device focused on the description in terms of the Landau Ï†4 theory for the critical phenomena in finite systems. This further allowed the utilization of the property of the anomalous enhancement of the autocorrelation function when a system is on the Spontaneous Symmetry Breaking (SSB), for improving the quantity of the demonstrated memory, while simultaneously maintaining a very good quality, as this is expressed by the stability of the autocorrelation function. In this proof-of-concept case, the morphology of the signal allowed us to impose the appropriate modifications on the signal so that we finally show how to get very close to the characteristics of the SSB and thus achieve our goal to get as close as possible to the ideal behavior of a Memristor that yields deep memory. Finally, we provide proof of the stability of memristor's operation by showing that solitons "follow" as a skeleton structure the experimentally derived time series.

### [Dynamical stability by spin transfer in nearly isotropic magnets](http://arxiv.org/abs/2601.08738v1)
**2026-01-13** | *Hidekazu Kurebayashi, Joseph Barker, Takumi Yamazaki et al.*

> Spin transfer torques (STTs) control magnetisation by electric currents, enabling a range of nano-scale spintronic applications. They can destabilise the equilibrium magnetisation state by counteracting magnetic relaxation. Here, we maximise the STT effect through a dedicated growth-annealing protocol for CoFeB thin films, such that magnetic anisotropies originating from the interface and shape almost cancel each other. The nearly isotropic magnets enable low-current dynamical stabilisation of the magnetisation in the direction opposite to an applied magnetic field, thereby realising a spintronic analogue of the Kapitza pendulum. In an intermediate current regime, the STT drives large magnetisation vector fluctuations that cover the entire Bloch sphere. The continuous variable associated with the stochastic magnetisation direction may serve as a resource for probabilistic computing and neuromorphic hardware. Our results establish isotropic magnets as a platform to study as-yet-uncharted, far-from-equilibrium spin dynamics including anti-magnonics, with promising implications for unconventional computing paradigms.

### [Neuromorphic FPGA Design for Digital Signal Processing](http://arxiv.org/abs/2601.07069v1)
**2026-01-11** | *Justin London*

> In this paper, the foundations of neuromorphic computing, spiking neural networks (SNNs) and memristors, are analyzed and discussed. Neuromorphic computing is then applied to FPGA design for digital signal processing (DSP). Finite impulse response (FIR) and infinite impulse response (IIR) filters are implemented with and without neuromorphic computing in Vivado using Verilog HDL. The results suggest that neuromorphic computing can provide low-latency and synaptic plasticity thereby enabling continuous on-chip learning. Due to their parallel and event-driven nature, neuromorphic computing can reduce power consumption by eliminating von Neumann bottlenecks and improve efficiency, but at the cost of reduced numeric precision.

### [Thermally Configurable Multi-Order Polar Skyrmions in Multiferroic Oxide Superlattices](http://arxiv.org/abs/2601.05950v1)
**2026-01-09** | *Kefan Liu, Yuhui Huang, Xiangwei Guo et al.*

> Polar topological textures in low-dimensional ferroelectrics have emerged as a versatile platform for high-density information storage and neuromorphic computing. While low-order topological states, such as vortices and skyrmions, have been extensively studied, high-order polar topological families remain largely unexplored due to their higher energy requirements and limited stabilization methods. Here, using a BiFeO3 (BFO)-based multiferroic superlattice as a model system, we demonstrate a thermal-modulation strategy that stabilizes multi-order polar skyrmions and enables reversible tuning of their topological order through phase-field simulations. It was found that temperature modulation drives the system from polar solitons through 1Ï€-, 2Ï€-, 3Ï€-, and 4Ï€-skyrmion states, with closed heating-cooling path analyses revealing the widest thermal stability window for 2Ï€-skyrmions (up to 600 K). Leveraging this robustness, 2% Sm doping in BFO lowers the transition temperatures, enabling room-temperature stabilization of 2Ï€-skyrmions. These findings enrich the fundamental understanding of multi-order polar topologies and establish a tunable strategy for realizing variable-order topological configurations in practical memory devices.

### [Self-Evolving Distributed Memory Architecture for Scalable AI Systems](http://arxiv.org/abs/2601.05569v1)
**2026-01-09** | *Zixuan Li, Chuanzhen Wang, Haotian Sun*

> Distributed AI systems face critical memory management challenges across computation, communication, and deployment layers. RRAM based in memory computing suffers from scalability limitations due to device non idealities and fixed array sizes. Decentralized AI frameworks struggle with memory efficiency across NAT constrained networks due to static routing that ignores computational load. Multi agent deployment systems tightly couple application logic with execution environments, preventing adaptive memory optimization. These challenges stem from a fundamental lack of coordinated memory management across architectural layers. We introduce Self Evolving Distributed Memory Architecture for Scalable AI Systems, a three layer framework that unifies memory management across computation, communication, and deployment. Our approach features (1) memory guided matrix processing with dynamic partitioning based on device characteristics, (2) memory aware peer selection considering network topology and computational capacity, and (3) runtime adaptive deployment optimization through continuous reconfiguration. The framework maintains dual memory systems tracking both long term performance patterns and short term workload statistics. Experiments on COCO 2017, ImageNet, and SQuAD show that our method achieves 87.3 percent memory utilization efficiency and 142.5 operations per second compared to Ray Distributed at 72.1 percent and 98.7 operations per second, while reducing communication latency by 30.2 percent to 171.2 milliseconds and improving resource utilization to 82.7 percent. Our contributions include coordinated memory management across three architectural layers, workload adaptive resource allocation, and a dual memory architecture enabling dynamic system optimization.

---

## ðŸ§  Algorithms & Theory

### [Effects of Introducing Synaptic Scaling on Spiking Neural Network Learning](http://arxiv.org/abs/2601.11261v1)
**2026-01-16** | *Shinnosuke Touda, Hirotsugu Okuno*

> Spiking neural networks (SNNs) employing unsupervised learning methods inspired by neural plasticity are expected to be a new framework for artificial intelligence. In this study, we investigated the effect of multiple types of neural plasticity, such as spike-time-dependent plasticity (STDP) and synaptic scaling, on the learning in a winner-take-all (WTA) network composed of spiking neurons. We implemented a WTA network with multiple types of neural plasticity using Python. The MNIST and the Fashion-MNIST datasets were used for training and testing. We varied the number of neurons, the time constant of STDP, and the normalization method used in synaptic scaling to compare classification accuracy. The results demonstrated that synaptic scaling based on the L2 norm was the most effective in improving classification performance. By implementing L2-norm-based synaptic scaling and setting the number of neurons in both excitatory and inhibitory layers to 400, the network achieved classification accuracies of 88.84 % on the MNIST dataset and 68.01 % on the Fashion-MNIST dataset after one epoch of training.

### [Supervised Spike Agreement Dependent Plasticity for Fast Local Learning in Spiking Neural Networks](http://arxiv.org/abs/2601.08526v1)
**2026-01-13** | *Gouri Lakshmi S, Athira Chandrasekharan, Harshit Kumar et al.*

> Spike-Timing-Dependent Plasticity (STDP) provides a biologically grounded learning rule for spiking neural networks (SNNs), but its reliance on precise spike timing and pairwise updates limits fast learning of weights. We introduce a supervised extension of Spike Agreement-Dependent Plasticity (SADP), which replaces pairwise spike-timing comparisons with population-level agreement metrics such as Cohen's kappa. The proposed learning rule preserves strict synaptic locality, admits linear-time complexity, and enables efficient supervised learning without backpropagation, surrogate gradients, or teacher forcing.   We integrate supervised SADP within hybrid CNN-SNN architectures, where convolutional encoders provide compact feature representations that are converted into Poisson spike trains for agreement-driven learning in the SNN. Extensive experiments on MNIST, Fashion-MNIST, CIFAR-10, and biomedical image classification tasks demonstrate competitive performance and fast convergence. Additional analyses show stable performance across broad hyperparameter ranges and compatibility with device-inspired synaptic update dynamics. Together, these results establish supervised SADP as a scalable, biologically grounded, and hardware-aligned learning paradigm for spiking neural networks.

### [Sleep-Based Homeostatic Regularization for Stabilizing Spike-Timing-Dependent Plasticity in Recurrent Spiking Neural Networks](http://arxiv.org/abs/2601.08447v1)
**2026-01-13** | *Andreas Massey, Aliaksandr Hubin, Stefano Nichele et al.*

> Spike-timing-dependent plasticity (STDP) provides a biologically-plausible learning mechanism for spiking neural networks (SNNs); however, Hebbian weight updates in architectures with recurrent connections suffer from pathological weight dynamics: unbounded growth, catastrophic forgetting, and loss of representational diversity. We propose a neuromorphic regularization scheme inspired by the synaptic homeostasis hypothesis: periodic offline phases during which external inputs are suppressed, synaptic weights undergo stochastic decay toward a homeostatic baseline, and spontaneous activity enables memory consolidation. We demonstrate that this sleep-wake cycle prevents weight saturation while preserving learned structure. Empirically, we find that low to intermediate sleep durations (10-20\% of training) improve stability on MNIST-like benchmarks in our STDP-SNN model, without any data-specific hyperparameter tuning. In contrast, the same sleep intervention yields no measurable benefit for the surrogate-gradient spiking neural network (SG-SNN). Taken together, these results suggest that periodic, sleep-based renormalization may represent a fundamental mechanism for stabilizing local Hebbian learning in neuromorphic systems, while also indicating that special care is required when integrating such protocols with existing gradient-based optimization methods.

### [Spiking Neural-Invariant Kalman Fusion for Accurate Localization Using Low-Cost IMUs](http://arxiv.org/abs/2601.08248v1)
**2026-01-13** | *Yaohua Liu, Qiao Xu, Yemin Wang et al.*

> Low-cost inertial measurement units (IMUs) are widely utilized in mobile robot localization due to their affordability and ease of integration. However, their complex, nonlinear, and time-varying noise characteristics often lead to significant degradation in localization accuracy when applied directly for dead reckoning. To overcome this limitation, we propose a novel brain-inspired state estimation framework that combines a spiking neural network (SNN) with an invariant extended Kalman filter (InEKF). The SNN is designed to extract motion-related features from long sequences of IMU data affected by substantial random noise and is trained via a surrogate gradient descent algorithm to enable dynamic adaptation of the covariance noise parameter within the InEKF. By fusing the SNN output with raw IMU measurements, the proposed method enhances the robustness and accuracy of pose estimation. Extensive experiments conducted on the KITTI dataset and real-world data collected using a mobile robot equipped with a low-cost IMU demonstrate that the proposed approach outperforms state-of-the-art methods in localization accuracy and exhibits strong robustness to sensor noise, highlighting its potential for real-world mobile robot applications.

### [A brain-inspired information fusion method for enhancing robot GPS outages navigation](http://arxiv.org/abs/2601.08244v1)
**2026-01-13** | *Yaohua Liu, Hengjun Zhang, Binkai Ou*

> Low-cost inertial navigation systems (INS) are prone to sensor biases and measurement noise, which lead to rapid degradation of navigation accuracy during global positioning system (GPS) outages. To address this challenge and improve positioning continuity in GPS-denied environments, this paper proposes a brain-inspired GPS/INS fusion network (BGFN) based on spiking neural networks (SNNs). The BGFN architecture integrates a spiking Transformer with a spiking encoder to simultaneously extract spatial features from inertial measurement unit (IMU) signals and capture their temporal dynamics. By modeling the relationship between vehicle attitude, specific force, angular rate, and GPS-derived position increments, the network leverages both current and historical IMU data to estimate vehicle motion. The effectiveness of the proposed method is evaluated through real-world field tests and experiments on public datasets. Compared to conventional deep learning approaches, the results demonstrate that BGFN achieves higher accuracy and enhanced reliability in navigation performance, particularly under prolonged GPS outages.

### [The Potential Impact of Neuromorphic Computing on Radio Telescope Observatories](http://arxiv.org/abs/2601.07130v1)
**2026-01-12** | *Nicholas J. Pritchard, Richard Dodson, Andreas Wicenec*

> Radio astronomy relies on bespoke, experimental and innovative computing solutions. This will continue as next-generation telescopes such as the Square Kilometre Array (SKA) and next-generation Very Large Array (ngVLA) take shape. Under increasingly demanding power consumption, and increasingly challenging radio environments, science goals may become intractable with conventional von Neumann computing due to related power requirements. Neuromorphic computing offers a compelling alternative, and combined with a desire for data-driven methods, Spiking Neural Networks (SNNs) are a promising real-time power-efficient alternative. Radio Frequency Interference (RFI) detection is an attractive use-case for SNNs where recent exploration holds promise. This work presents a comprehensive analysis of the potential impact of deploying varying neuromorphic approaches across key stages in radio astronomy processing pipelines for several existing and near-term instruments. Our analysis paves a realistic path from near-term FPGA deployment of SNNs in existing instruments, allowing the addition of advanced data-driven RFI detection for no capital cost, to neuromorphic ASICs for future instruments, finding that commercially available solutions could reduce the power budget for key processing elements by up to three orders of magnitude, transforming the operational budget of the observatory. High-data-rate spectrographic processing could be a well-suited target for the neuromorphic computing industry, as we cast radio telescopes as the world's largest in-sensor compute challenge.

### [Efficient Aspect Term Extraction using Spiking Neural Network](http://arxiv.org/abs/2601.06637v1)
**2026-01-10** | *Abhishek Kumar Mishra, Arya Somasundaram, Anup Das et al.*

> Aspect Term Extraction (ATE) identifies aspect terms in review sentences, a key subtask of sentiment analysis. While most existing approaches use energy-intensive deep neural networks (DNNs) for ATE as sequence labeling, this paper proposes a more energy-efficient alternative using Spiking Neural Networks (SNNs). Using sparse activations and event-driven inferences, SNNs capture temporal dependencies between words, making them suitable for ATE. The proposed architecture, SpikeATE, employs ternary spiking neurons and direct spike training fine-tuned with pseudo-gradients. Evaluated on four benchmark SemEval datasets, SpikeATE achieves performance comparable to state-of-the-art DNNs with significantly lower energy consumption. This highlights the use of SNNs as a practical and sustainable choice for ATE tasks.

### [Nonlinear mode interactions under parametric excitation in a YIG microdisk](http://arxiv.org/abs/2601.05775v1)
**2026-01-09** | *Gabriel Soares, Rafael Lopes Seeger, Amel Kolli et al.*

> A pair of quantized spin-wave modes is driven by two-tone parallel pumping in a YIG microdisk. The nonlinear dynamics is experimentally investigated by probing the resulting steady state, which is found to critically depend on the chosen pair of modes, the detuning between the pump frequencies and the modes parametric resonance, as well as the temporal sequence of the two rf tones. A general theory of parametric excitation in confined structures based on magnetization normal modes is developed and quantitatively accounts for the observed dependence and non-commutative behaviors, which emerge from the interplay between the self and mutual nonlinear frequency shifts of the spin-wave modes. Owing to its high degree of external controllability and scalability to larger sets of modes, this dynamical system provides a model platform for exploring nonlinear phenomena and a promising route toward rf driven state mapping relevant to neuromorphic and unconventional computing.

### [EARL: Energy-Aware Optimization of Liquid State Machines for Pervasive AI](http://arxiv.org/abs/2601.05205v1)
**2026-01-08** | *Zain Iqbal, Lorenzo Valerio*

> Pervasive AI increasingly depends on on-device learning systems that deliver low-latency and energy-efficient computation under strict resource constraints. Liquid State Machines (LSMs) offer a promising approach for low-power temporal processing in pervasive and neuromorphic systems, but their deployment remains challenging due to high hyperparameter sensitivity and the computational cost of traditional optimization methods that ignore energy constraints. This work presents EARL, an energy-aware reinforcement learning framework that integrates Bayesian optimization with an adaptive reinforcement learning based selection policy to jointly optimize accuracy and energy consumption. EARL employs surrogate modeling for global exploration, reinforcement learning for dynamic candidate prioritization, and an early termination mechanism to eliminate redundant evaluations, substantially reducing computational overhead. Experiments on three benchmark datasets demonstrate that EARL achieves 6 to 15 percent higher accuracy, 60 to 80 percent lower energy consumption, and up to an order of magnitude reduction in optimization time compared to leading hyperparameter tuning frameworks. These results highlight the effectiveness of energy-aware adaptive search in improving the efficiency and scalability of LSMs for resource-constrained on-device AI applications.

---

## ðŸ‘ï¸ Applications & Sensing

### [Optical Linear Systems Framework for Event Sensing and Computational Neuromorphic Imaging](http://arxiv.org/abs/2601.13498v1)
**2026-01-20** | *Nimrod Kruger, Nicholas Owen Ralph, Gregory Cohen et al.*

> Event vision sensors (neuromorphic cameras) output sparse, asynchronous ON/OFF events triggered by log-intensity threshold crossings, enabling microsecond-scale sensing with high dynamic range and low data bandwidth. As a nonlinear system, this event representation does not readily integrate with the linear forward models that underpin most computational imaging and optical system design. We present a physics-grounded processing pipeline that maps event streams to estimates of per-pixel log-intensity and intensity derivatives, and embeds these measurements in a dynamic linear systems model with a time-varying point spread function. This enables inverse filtering directly from event data, using frequency-domain Wiener deconvolution with a known (or parameterised) dynamic transfer function. We validate the approach in simulation for single and overlapping point sources under modulated defocus, and on real event data from a tunable-focus telescope imaging a star field, demonstrating source localisation and separability. The proposed framework provides a practical bridge between event sensing and model-based computational imaging for dynamic optical systems.

### [Event-based Heterogeneous Information Processing for Online Vision-based Obstacle Detection and Localization](http://arxiv.org/abs/2601.13451v1)
**2026-01-19** | *Reza Ahmadvand, Sarah Safura Sharif, Yaser Mike Banad*

> This paper introduces a novel framework for robotic vision-based navigation that integrates Hybrid Neural Networks (HNNs) with Spiking Neural Network (SNN)-based filtering to enhance situational awareness for unmodeled obstacle detection and localization. By leveraging the complementary strengths of Artificial Neural Networks (ANNs) and SNNs, the system achieves both accurate environmental understanding and fast, energy-efficient processing. The proposed architecture employs a dual-pathway approach: an ANN component processes static spatial features at low frequency, while an SNN component handles dynamic, event-based sensor data in real time. Unlike conventional hybrid architectures that rely on domain conversion mechanisms, our system incorporates a pre-developed SNN-based filter that directly utilizes spike-encoded inputs for localization and state estimation. Detected anomalies are validated using contextual information from the ANN pathway and continuously tracked to support anticipatory navigation strategies. Simulation results demonstrate that the proposed method offers acceptable detection accuracy while maintaining computational efficiency close to SNN-only implementations, which operate at a fraction of the resource cost. This framework represents a significant advancement in neuromorphic navigation systems for robots operating in unpredictable and dynamic environments.

### [Autonomous Navigation at the Nano-Scale: Algorithms, Architectures, and Constraints](http://arxiv.org/abs/2601.13252v1)
**2026-01-19** | *Mahmud S. Zango, Jianglin Lan*

> Autonomous navigation for nano-scale unmanned aerial vehicles (nano-UAVs) is governed by extreme Size, Weight, and Power (SWaP) constraints (with the weight < 50 g and sub-100 mW onboard processor), distinguishing it fundamentally from standard robotic paradigms. This review synthesizes the state-of-the-art in sensing, computing, and control architectures designed specifically for these sub- 100mW computational envelopes. We critically analyse the transition from classical geometry-based methods to emerging "Edge AI" paradigms, including quantized deep neural networks deployed on ultra-low-power System-on-Chips (SoCs) and neuromorphic event-based control. Beyond algorithms, we evaluate the hardware-software co-design requisite for autonomy, covering advancements in dense optical flow, optimized Simultaneous Localization and Mapping (SLAM), and learning-based flight control. While significant progress has been observed in visual navigation and relative pose estimation, our analysis reveals persistent gaps in long-term endurance, robust obstacle avoidance in dynamic environments, and the "Sim-to-Real" transfer of reinforcement learning policies. This survey provides a roadmap for bridging these gaps, advocating for hybrid architectures that fuse lightweight classical control with data-driven perception to enable fully autonomous, agile nano-UAVs in GPS-denied environments.

### [UEOF: A Benchmark Dataset for Underwater Event-Based Optical Flow](http://arxiv.org/abs/2601.10054v1)
**2026-01-15** | *Nick Truong, Pritam P. Karmokar, William J. Beksi*

> Underwater imaging is fundamentally challenging due to wavelength-dependent light attenuation, strong scattering from suspended particles, turbidity-induced blur, and non-uniform illumination. These effects impair standard cameras and make ground-truth motion nearly impossible to obtain. On the other hand, event cameras offer microsecond resolution and high dynamic range. Nonetheless, progress on investigating event cameras for underwater environments has been limited due to the lack of datasets that pair realistic underwater optics with accurate optical flow. To address this problem, we introduce the first synthetic underwater benchmark dataset for event-based optical flow derived from physically-based ray-traced RGBD sequences. Using a modern video-to-event pipeline applied to rendered underwater videos, we produce realistic event data streams with dense ground-truth flow, depth, and camera motion. Moreover, we benchmark state-of-the-art learning-based and model-based optical flow prediction methods to understand how underwater light transport affects event formation and motion estimation accuracy. Our dataset establishes a new baseline for future development and evaluation of underwater event-based perception algorithms. The source code and dataset for this project are publicly available at https://robotic-vision-lab.github.io/ueof.

### [Hybrid guided variational autoencoder for visual place recognition](http://arxiv.org/abs/2601.09248v1)
**2026-01-14** | *Ni Wang, Zihan You, Emre Neftci et al.*

> Autonomous agents such as cars, robots and drones need to precisely localize themselves in diverse environments, including in GPS-denied indoor environments. One approach for precise localization is visual place recognition (VPR), which estimates the place of an image based on previously seen places. State-of-the-art VPR models require high amounts of memory, making them unwieldy for mobile deployment, while more compact models lack robustness and generalization capabilities. This work overcomes these limitations for robotics using a combination of event-based vision sensors and an event-based novel guided variational autoencoder (VAE). The encoder part of our model is based on a spiking neural network model which is compatible with power-efficient low latency neuromorphic hardware. The VAE successfully disentangles the visual features of 16 distinct places in our new indoor VPR dataset with a classification performance comparable to other state-of-the-art approaches while, showing robust performance also under various illumination conditions. When tested with novel visual inputs from unknown scenes, our model can distinguish between these places, which demonstrates a high generalization capability by learning the essential features of location. Our compact and robust guided VAE with generalization capabilities poses a promising model for visual place recognition that can significantly enhance mobile robot navigation in known and unknown indoor environments.

### [Heterogeneous computing platform for real-time robotics](http://arxiv.org/abs/2601.09755v1)
**2026-01-13** | *Jakub Fil, Yulia Sandamirskaya, Hector Gonzalez et al.*

> After Industry 4.0 has embraced tight integration between machinery (OT), software (IT), and the Internet, creating a web of sensors, data, and algorithms in service of efficient and reliable production, a new concept of Society 5.0 is emerging, in which infrastructure of a city will be instrumented to increase reliability, efficiency, and safety. Robotics will play a pivotal role in enabling this vision that is pioneered by the NEOM initiative - a smart city, co-inhabited by humans and robots. In this paper we explore the computing platform that will be required to enable this vision. We show how we can combine neuromorphic computing hardware, exemplified by the Loihi2 processor used in conjunction with event-based cameras, for sensing and real-time perception and interaction with a local AI compute cluster (GPUs) for high-level language processing, cognition, and task planning. We demonstrate the use of this hybrid computing architecture in an interactive task, in which a humanoid robot plays a musical instrument with a human. Central to our design is the efficient and seamless integration of disparate components, ensuring that the synergy between software and hardware maximizes overall performance and responsiveness. Our proposed system architecture underscores the potential of heterogeneous computing architectures in advancing robotic autonomy and interactive intelligence, pointing toward a future where such integrated systems become the norm in complex, real-time applications.

### [Line-based Event Preprocessing: Towards Low-Energy Neuromorphic Computer Vision](http://arxiv.org/abs/2601.10742v1)
**2026-01-10** | *AmÃ©lie Gruel, Pierre Lewden, Adrien F. Vincent et al.*

> Neuromorphic vision made significant progress in recent years, thanks to the natural match between spiking neural networks and event data in terms of biological inspiration, energy savings, latency and memory use for dynamic visual data processing. However, optimising its energy requirements still remains a challenge within the community, especially for embedded applications. One solution may reside in preprocessing events to optimise data quantity thus lowering the energy cost on neuromorphic hardware, proportional to the number of synaptic operations. To this end, we extend an end-to-end neuromorphic line detection mechanism to introduce line-based event data preprocessing. Our results demonstrate on three benchmark event-based datasets that preprocessing leads to an advantageous trade-off between energy consumption and classification performance. Depending on the line-based preprocessing strategy and the complexity of the classification task, we show that one can maintain or increase the classification accuracy while significantly reducing the theoretical energy consumption. Our approach systematically leads to a significant improvement of the neuromorphic classification efficiency, thus laying the groundwork towards a more frugal neuromorphic computer vision thanks to event preprocessing.

---

## ðŸ“‚ General / Uncategorized

### [Annotated PIM Bibliography](http://arxiv.org/abs/2601.09002v1)
**2026-01-13** | *Peter M. Kogge*

> Processing in Memory (PIM) and similar terms such as Compute In Memory (CIM), Logic in Memory (LIM), In Memory Computing (IMC), and Near Memory Computing (NMC) have gained attention recently as a potentially ``revolutionary new'' technique. The truth, however, is that many examples of the technology go back over 60 years. This document attempts to provide an annotated bibliography of PIM technology that attempts to cover the whole time-frame, and is organized to augment a forth-coming article.

---

