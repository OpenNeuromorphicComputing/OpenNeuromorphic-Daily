# üß† Open Neuromorphic - Daily ArXiv

**Automated Daily Update** | Last Run: 2026-02-06 08:40 UTC

Papers are automatically categorized by topic and sorted by date.

## üõ† Hardware & Materials

### [Photonic neuromorphic processing with coupled spiking silicon microrings](http://arxiv.org/abs/2602.05918v1)
**2026-02-05** | *Giovanni Donati, Stefano Biasi, Lorenzo Pavesi et al.*

> Understanding the physical computing mechanisms of individual network nodes is essential for scaling neuromorphic photonic architectures. This work proposes a compact passive nonlinear photonic core based on a Side-Coupled Integrated Spaced Sequence of Resonators (SCISSOR) made of three nominally equal microrings and investigate its computing capabilities. Its nonlinearities and internal feedback enable analogue, spiking, and bistable responses that are accessed by tuning the injection power and wavelength. Implemented as a single nonlinear node in a time-multiplexed reservoir computing, the SCISSOR achieves error-free classification on the Iris dataset and accuracies above 97% on the Sonar task, using both analogue and digital reservoir representations with 150 virtual nodes. In the digital scheme, spiking dynamics naturally generate sparse reservoir states, enabling efficient classification even with a single spike. Intriguingly, optimal operating points are at the boundaries where sharp transitions in dynamical complexity and/or output power occur. In these points, the SCISSOR supports high task-performance, opening novel strategies for future on-chip training. Spiking and thermal bistabilities also participate to enhance the computational performance at low injected powers below 4 mW. These results suggest optical coupled microring resonators as effective building blocks for future edge computing and neuromorphic photonic systems.

### [Compact, Reconfigurable Optical Delay Line on a Bent Silica Fiber](http://arxiv.org/abs/2602.05757v1)
**2026-02-05** | *Manuel Crespo-Ballesteros, Misha Sumetsky*

> Tunable optical delay lines that simultaneously offer nanosecond-scale delay, broadband operation, low dispersion, and compact footprint remain challenging to realize with conventional integrated photonic platforms. Here we demonstrate a mechanically reconfigurable slow-light delay line based on a surface nanoscale axial photonics (SNAP) microresonator dynamically induced by controlled bending of a silica optical fiber. A localized nanoscale cutoff-wavelength dip, introduced by CO2-laser annealing, provides a reflective boundary, while fiber bending generates a smooth axial potential whose shape is continuously tunable via loop curvature. By adjusting the bending radius, the induced SNAP microresonator evolves from a nearly linear to an approximately semiparabolic axial profile, enabling a controlled transition from dispersive to nearly dispersionless delay. Using a transverse microfiber coupler operated at the impedance-matching condition, we experimentally demonstrate continuous delay tuning from 2 ns to 0.5 ns within a 10 GHz bandwidth in an approximately 2 mm long fiber segment, with insertion loss below 6 dB. The results confirm that mechanically induced SNAP microresonators provide a compact, robust, and reconfigurable platform for dispersion-engineered optical delay lines, with direct relevance to photonic beamforming, frequency-comb stabilization, and neuromorphic photonic signal processing.

### [Strong radial electric field scaling near nanoscale conductive filaments and the ReRAM resistive switching mechanism](http://arxiv.org/abs/2602.05067v1)
**2026-02-04** | *Robin Jacobs-Gedrim, William Wahby, Thomas Awe et al.*

> The physics underlying reset in bipolar resistive memory has been the subject of decades of controversy and has been identified as the primary barrier to resistive memory technology development. This manuscript introduces a nanoscale effect in current carrying conductors, whereby surface charge induced radial electric fields are found to be inversely proportional to the radius of the conductive path. This nanoscale effect is then applied to explain the negative resistance switching (reset) mechanism in filamentary metal oxide resistive switching memory devices (memristors). Previous explanations for the negative resistive switching mechanism state that diffusion constitutes the radial driving mechanism for oxygen ions, and drift under electric fields is restricted to the direction parallel to current flow. This explanation conflicts with retention and microscopy data collected in a subset of devices presented in literature. We demonstrate that the electric field's dependency on the on the radius of a nanoscale conductive path can result in radial fields on the order of 10^5 to 10^6 V/cm at only -1 V bias, sufficient to govern the negative resistance switching mechanism in filamentary metal oxides. By accounting for this nanoscale size effect, long standing anomalous experimental data about the negative (reset) resistance switching mechanism in bipolar filamentary resistive memory devices is finally reconciled. Wide understanding of surface charges and associated electric fields in nanoscale conductive paths could prove important for further scaling of integrated circuits and aid in elucidating many nanoscale phenomena.

### [Evolutionary Mapping of Neural Networks to Spatial Accelerators](http://arxiv.org/abs/2602.04717v1)
**2026-02-04** | *Alessandro Pierro, Jonathan Timcheck, Jason Yik et al.*

> Spatial accelerators, composed of arrays of compute-memory integrated units, offer an attractive platform for deploying inference workloads with low latency and low energy consumption. However, fully exploiting their architectural advantages typically requires careful, expert-driven mapping of computational graphs to distributed processing elements. In this work, we automate this process by framing the mapping challenge as a black-box optimization problem. We introduce the first evolutionary, hardware-in-the-loop mapping framework for neuromorphic accelerators, enabling users without deep hardware knowledge to deploy workloads more efficiently. We evaluate our approach on Intel Loihi 2, a representative spatial accelerator featuring 152 cores per chip in a 2D mesh. Our method achieves up to 35% reduction in total latency compared to default heuristics on two sparse multi-layer perceptron networks. Furthermore, we demonstrate the scalability of our approach to multi-chip systems and observe an up to 40% improvement in energy efficiency, without explicitly optimizing for it.

### [Scalable platform enabling reservoir computing with nanoporous oxide memristors for image recognition and time series prediction](http://arxiv.org/abs/2602.04619v1)
**2026-02-04** | *Joshua Donald, Ben A. Johnson, Amir Mehrnejat et al.*

> Typical mammal brains have some form of random connectivity between neurons. Reservoir computing, a neural network approach, uses random weights within its processing layer along with built-in recurrent connections and short-term, fading memory, and is shown to be time and training efficient in processing spatiotemporal signals. Here we prepared a niobium oxide-based thin film memristor device with intrinsic structural in-homogeneity in the form of random nanopores and performed computational tasks of XOR operations, image recognition, and time series prediction and reconstruction. For the latter task we chose a complex three-dimensional chaotic Lorenz-63 time series. By applying three temporal voltage waveforms individually across the device and training the readout layer with electrical current signals from a three-output physical reservoir, we achieved satisfactory prediction and reconstruction accuracy in comparison to the case of no reservoir. This work highlights the potential for scalable, on-chip devices using all-oxide reservoir systems, paving the way for energy-efficient neuromorphic electronics dealing with time signals.

### [Real-time processing of analog signals on accelerated neuromorphic hardware](http://arxiv.org/abs/2602.04582v1)
**2026-02-04** | *Yannik Stradmann, Johannes Schemmel, Mihai A. Petrovici et al.*

> Sensory processing with neuromorphic systems is typically done by using either event-based sensors or translating input signals to spikes before presenting them to the neuromorphic processor. Here, we offer an alternative approach: direct analog signal injection eliminates superfluous and power-intensive analog-to-digital and digital-to-analog conversions, making it particularly suitable for efficient near-sensor processing. We demonstrate this by using the accelerated BrainScaleS-2 mixed-signal neuromorphic research platform and interfacing it directly to microphones and a servo-motor-driven actuator. Utilizing BrainScaleS-2's 1000-fold acceleration factor, we employ a spiking neural network to transform interaural time differences into a spatial code and thereby predict the location of sound sources. Our primary contributions are the first demonstrations of direct, continuous-valued sensor data injection into the analog compute units of the BrainScaleS-2 ASIC, and actuator control using its embedded microprocessors. This enables a fully on-chip processing pipeline$\unicode{x2014}$from sensory input handling, via spiking neural network processing to physical action. We showcase this by programming the system to localize and align a servo motor with the spatial direction of transient noise peaks in real-time.

### [A Comparative Study of Digital Memristor-Based Processing-In-Memory from a Device and Reliability Perspective](http://arxiv.org/abs/2602.04035v1)
**2026-02-03** | *Thomas Neuner, Henriette Padberg, Lior Kornblum et al.*

> As data-intensive applications increasingly strain conventional computing systems, processing-in-memory (PIM) has emerged as a promising paradigm to alleviate the memory wall by minimizing data transfer between memory and processing units. This review presents the recent advances in both stateful and non-stateful logic techniques for PIM, focusing on emerging nonvolatile memory technologies such as resistive random-access memory (RRAM), phase-change memory (PCM), and magnetoresistive random-access memory (MRAM). Both experimentally demonstrated and simulated logic designs are critically examined, highlighting key challenges in reliability and the role of device-level optimization in enabling scalable and commercial viable PIM systems. The review begins with an overview of relevant logic families, memristive device types, and associated reliability metrics. Each logic family is then explored in terms of how it capitalizes on distinct device properties to implement logic techniques. A comparative table of representative device stacks and performance parameters illustrates trade-offs and quality indicators. Through this comprehensive analysis, the development of optimized, robust memristive devices for next-generation PIM applications is supported.

### [Stochastic Dynamics of Diffusive Memristor Blocks for Neuromorphic Computing](http://arxiv.org/abs/2602.03700v1)
**2026-02-03** | *Wendy Otieno, Alex Gabbitas, Debi Pattnaik et al.*

> Biological systems use neural circuits to integrate input information and produce outputs. Synaptic convergence, where multiple neurons converge their inputs onto a single downstream neuron, is common in natural neural circuits. However, understanding specific computations performed by such neural blocks and implementating them in hardware requires further research. This work focuses on synaptic convergence in a simplified circuit of three spiking artificial neurons based on diffusive memristors. Numerical modelling and experiments reveal input voltage combinations that enable targeted activation of spiking for specific neuron configurations. We analyse the statistical characteristics of spiking patterns and interpret them from a computational perspective. The numerical simulations match experimental measurements. Our findings contribute to development of universal functional blocks for neuromorphic systems.

### [Towards Polyoxometalate Nanoelectronics](http://arxiv.org/abs/2602.03512v1)
**2026-02-03** | *Dominique Vuillaume, Anna Proust*

> Polyoxometalates form a large family of molecular oxide clusters of the early transition metals with unique and tunable properties (multi-redox, thermal and chemical robustness, magnetic). We review more than 30 years of experimental research on the electron transport properties of polyoxometalates devices, from thin films and self-assembled monolayers down to single-molecule junctions. We focus on the relationship between the polyoxometalate structures (structural type, nature of metals and heteroatoms, role of the counterions, redox states, electrode linkers and functional ligands) and the electronic structures of the polyoxometalate-based devices (energy positions of the molecular orbitals, energy offset at the interfaces). Then, we critically discuss the performances of polyoxometalates in nanoelectronics devices: capacitance and resistive switching memories, spintronics, quantum bits and neuromorphic devices. We conclude with a discussion about pending issues and perspectives.

### [Spin splitting torque enabled artificial neuron with self-reset via synthetic antiferromagnetic coupling](http://arxiv.org/abs/2602.01874v1)
**2026-02-02** | *Badsha Sekh, Hasibur Rahaman, Ravi Shankar Verma et al.*

> Spintronic artificial neurons are intriguing building blocks for energy efficient Neuromorphic Computing (NC). Nevertheless, most contemporary implementations rely on symmetry breaking external in plane magnetic fields (H_X) for neuron operation, which limits scalability and hardware practicality. We experimentally demonstrate an altermagnet/Synthetic Antiferromagnetic Coupling (SAF) based spintronic neuron that uses out of plane spin (œÉ_Z) polarized spin-splitting torque to eliminate the necessity of an external H_X. The neuron device also features intrinsic self-reset function facilitated by built-in exchange coupling. Furthermore, the proposed device is validated for Spiking Neural Network (SNN) applications by achieving test accuracies of 95.99% and 94.36% on the MNIST and N-MNIST datasets, respectively. These results demonstrate the hardware feasibility and compatibility of the proposed spintronic neuron, highlighting its potential for compact, scalable and energy-efficient neuromorphic computing systems.

### [In-Pipeline Integration of Digital In-Memory-Computing into RISC-V Vector Architecture to Accelerate Deep Learning](http://arxiv.org/abs/2602.01827v1)
**2026-02-02** | *Tommaso Spagnolo, Cristina Silvano, Riccardo Massa et al.*

> Expanding Deep Learning applications toward edge computing demands architectures capable of delivering high computational performance and efficiency while adhering to tight power and memory constraints. Digital In-Memory Computing (DIMC) addresses this need by moving part of the computation directly within memory arrays, significantly reducing data movement and improving energy efficiency. This paper introduces a novel architecture that extends the Vector RISC-V Instruction Set Architecture (ISA) to integrate a tightly coupled DIMC unit directly into the execution stage of the pipeline, to accelerate Deep Learning inference at the edge. Specifically, the proposed approach adds four custom instructions dedicated to data loading, computation, and write-back, enabling flexible and optimal control of the inference execution on the target architecture. Experimental results demonstrate high utilization of the DIMC tile in Vector RISC-V and sustained throughput across the ResNet-50 model, achieving a peak performance of 137 GOP/s. The proposed architecture achieves a speedup of 217x over the baseline core and 50x area-normalized speedup even when operating near the hardware resource limits. The experimental results confirm the high potential of the proposed architecture as a scalable and efficient solution to accelerate Deep Learning inference on the edge.

### [NeuroAI Temporal Neural Networks (NeuTNNs): Microarchitecture and Design Framework for Specialized Neuromorphic Processing Units](http://arxiv.org/abs/2602.01546v1)
**2026-02-02** | *Shanmuga Venkatachalam, Prabhu Vellaisamy, Harideep Nair et al.*

> Leading experts from both communities have suggested the need to (re)connect research in neuroscience and artificial intelligence (AI) to accelerate the development of next-generation AI innovations. They term this convergence as NeuroAI. Previous research has established temporal neural networks (TNNs) as a promising neuromorphic approach toward biological intelligence and efficiency. We fully embrace NeuroAI and propose a new category of TNNs we call NeuroAI TNNs (NeuTNNs) with greater capability and hardware efficiency by adopting neuroscience findings, including a neuron model with active dendrites and a hierarchy of distal and proximal segments. This work introduces a PyTorch-to-layout tool suite (NeuTNNGen) to design application-specific NeuTNNs. Compared to previous TNN designs, NeuTNNs achieve superior performance and efficiency. We demonstrate NeuTNNGen's capabilities using three example applications: 1) UCR time series benchmarks, 2) MNIST design exploration, and 3) Place Cells design for neocortical reference frames. We also explore using synaptic pruning to further reduce synapse counts and hardware costs by 30-50% while maintaining model precision across diverse sensory modalities. NeuTNNGen can facilitate the design of application-specific energy-efficient NeuTNNs for the next generation of NeuroAI computing systems.

### [Hardware implementation of photonic neuromorphic autonomous navigation](http://arxiv.org/abs/2602.01079v1)
**2026-02-01** | *Yonghang Chen, Shuiying Xiang, Xintao Zeng et al.*

> Reinforcement learning (RL) is a core technology enabling the transition of artificial intelligence (AI) from perception to decision-making, but its deployment on conventional electronic hardware suffers from high latency and energy consumption imposed by the von Neumann architecture. Here, we propose a photonic spiking twin delayed deep deterministic policy gradient (TD3) reinforcement learning architecture for neuromorphic autonomous navigation and experimentally validate it on a distributed feedback laser with a saturable absorber (DFB-SA) array. The hybrid architecture integrates a photonic spiking Actor network with dual continuous-valued Critic networks, where the final nonlinear spiking activation layer of the Actor is deployed on the DFB-SA laser array. In autonomous navigation tasks, the system achieves an average reward of 58.22 plus-minus 17.29 and a success rate of 80% plus-minus 8.3%. Hardware-software co-inference demonstrates an estimated energy consumption of 0.78 nJ/inf and an ultra-low latency of 191.20 ps/inf, with co-inference error rates of 0.051% and 0.059% in task scenarios with and without obstacle interference, respectively. Simulations for error-activated channels show full agreement with the expected responses, validating the dynamic characteristics of the DFB-SA laser. The architecture shows strong potential for integration with large-scale photonic linear computing chips, enabling fully-functional photonic computation and low-power, low-latency neuromorphic autonomous navigation.

### [System-Level Performance Modeling of Photonic In-Memory Computing](http://arxiv.org/abs/2602.00892v1)
**2026-01-31** | *Jebacyril Arockiaraj, Sasindu Wijeratne, Sugeet Sunder et al.*

> Photonic in-memory computing is a high-speed, low-energy alternative to traditional transistor-based digital computing that utilizes high photonic operating frequencies and bandwidths. In this work, we develop a comprehensive system-level performance model for photonic in-memory computing, capturing the effects of key latency sources such as external memory access and opto-electronic conversion. We perform algorithm-to-hardware mapping across a range of workloads, including the Sod shock tube problem, Matricized Tensor Times Khatri-Rao Product (MTTKRP), and the Vlasov-Maxwell equation, to evaluate how the latencies impact real-world high-performance computing workloads. Our performance model shows that, while accounting for system overheads, a compact 1x256 bit single-wavelength photonic SRAM array, fabricated using the standard silicon photonics process by GlobalFoundries, sustains up to 1.5 TOPS, 0.9 TOPS, and 1.3 TOPS on the Sod shock tube problem, MTTKRP, and the Vlasov-Maxwell equation with an average energy efficiency of 2.5 TOPS/W.

### [An Open-Source Framework for Measurement and Analysis of Nanoscale Ionic Transport](http://arxiv.org/abs/2602.00806v1)
**2026-01-31** | *Yichao Wang, Munan Fang, Aziz Roshanbhai Lokhandwala et al.*

> Nanofluidic systems exploit nanometre-scale confinement in channels and pores to regulate ionic transport, enabling functionalities such as osmotic energy harvesting and neuromorphic ionic memory. Studying such confined transport requires both precise electrical instrumentation and careful data analysis, yet, in practice, measurements are still often taken with vendor software, exported as files, and processed later in separate environments. In this work, we bring these steps together in a unified Python-based framework built around three interoperable graphical user interfaces (GUIs) for nanochannel, nanopore and memristor experiments. The framework is organised into two functional parts, measurement and analysis. On the measurement side, two GUIs drive Keithley Source Meters to run continuous voltage sweeps and user-defined memristive pulse sequences, while providing live plots, configuration management and controlled shutdown routines. On the analysis side, a dedicated nanochannel and nanopore GUI reads raw I-V datasets, applies unit-consistent processing, extracts conductance and ion mobility, evaluates selectivity and osmotic power, and is complemented by a web-based calculator that performs the same mobility analysis without a local Python installation. All three GUIs are implemented in Python/Tkinter with modular plotting and logging layers so that flexible control sequences and physics-based post-processing share a common data format, improving reproducibility, timing stability and day-to-day efficiency in nanofluidic and electronic device studies.

---

## üß† Algorithms & Theory

### [Time Is All It Takes: Spike-Retiming Attacks on Event-Driven Spiking Neural Networks](http://arxiv.org/abs/2602.03284v1)
**2026-02-03** | *Yi Yu, Qixin Zhang, Shuhan Ye et al.*

> Spiking neural networks (SNNs) compute with discrete spikes and exploit temporal structure, yet most adversarial attacks change intensities or event counts instead of timing. We study a timing-only adversary that retimes existing spikes while preserving spike counts and amplitudes in event-driven SNNs, thus remaining rate-preserving. We formalize a capacity-1 spike-retiming threat model with a unified trio of budgets: per-spike jitter $\mathcal{B}_{\infty}$, total delay $\mathcal{B}_{1}$, and tamper count $\mathcal{B}_{0}$. Feasible adversarial examples must satisfy timeline consistency and non-overlap, which makes the search space discrete and constrained. To optimize such retimings at scale, we use projected-in-the-loop (PIL) optimization: shift-probability logits yield a differentiable soft retiming for backpropagation, and a strict projection in the forward pass produces a feasible discrete schedule that satisfies capacity-1, non-overlap, and the chosen budget at every step. The objective maximizes task loss on the projected input and adds a capacity regularizer together with budget-aware penalties, which stabilizes gradients and aligns optimization with evaluation. Across event-driven benchmarks (CIFAR10-DVS, DVS-Gesture, N-MNIST) and diverse SNN architectures, we evaluate under binary and integer event grids and a range of retiming budgets, and also test models trained with timing-aware adversarial training designed to counter timing-only attacks. For example, on DVS-Gesture the attack attains high success (over $90\%$) while touching fewer than $2\%$ of spikes under $\mathcal{B}_{0}$. Taken together, our results show that spike retiming is a practical and stealthy attack surface that current defenses struggle to counter, providing a clear reference for temporal robustness in event-driven SNNs. Code is available at https://github.com/yuyi-sd/Spike-Retiming-Attacks.

### [Energy-Efficient Neuromorphic Computing for Edge AI: A Framework with Adaptive Spiking Neural Networks and Hardware-Aware Optimization](http://arxiv.org/abs/2602.02439v1)
**2026-02-02** | *Olaf Yunus Laitinen Imanov, Derya Umut Kulali, Taner Yilmaz et al.*

> Edge AI applications increasingly require ultra-low-power, low-latency inference. Neuromorphic computing based on event-driven spiking neural networks (SNNs) offers an attractive path, but practical deployment on resource-constrained devices is limited by training difficulty, hardware-mapping overheads, and sensitivity to temporal dynamics. We present NeuEdge, a framework that combines adaptive SNN models with hardware-aware optimization for edge deployment. NeuEdge uses a temporal coding scheme that blends rate and spike-timing patterns to reduce spike activity while preserving accuracy, and a hardware-aware training procedure that co-optimizes network structure and on-chip placement to improve utilization on neuromorphic processors. An adaptive threshold mechanism adjusts neuron excitability from input statistics, reducing energy consumption without degrading performance. Across standard vision and audio benchmarks, NeuEdge achieves 91-96% accuracy with up to 2.3 ms inference latency on edge hardware and an estimated 847 GOp/s/W energy efficiency. A case study on an autonomous-drone workload shows up to 312x energy savings relative to conventional deep neural networks while maintaining real-time operation.

### [Spark: Modular Spiking Neural Networks](http://arxiv.org/abs/2602.02306v1)
**2026-02-02** | *Mario Franco, Carlos Gershenson*

> Nowadays, neural networks act as a synonym for artificial intelligence. Present neural network models, although remarkably powerful, are inefficient both in terms of data and energy. Several alternative forms of neural networks have been proposed to address some of these problems. Specifically, spiking neural networks are suitable for efficient hardware implementations. However, effective learning algorithms for spiking networks remain elusive, although it is suspected that effective plasticity mechanisms could alleviate the problem of data efficiency. Here, we present a new framework for spiking neural networks - Spark - built upon the idea of modular design, from simple components to entire models. The aim of this framework is to provide an efficient and streamlined pipeline for spiking neural networks. We showcase this framework by solving the sparse-reward cartpole problem with simple plasticity mechanisms. We hope that a framework compatible with traditional ML pipelines may accelerate research in the area, specifically for continuous and unbatched learning, akin to the one animals exhibit.

### [Backpropagation as Physical Relaxation: Exact Gradients in Finite Time](http://arxiv.org/abs/2602.02281v1)
**2026-02-02** | *Antonino Emanuele Scurria*

> Backpropagation, the foundational algorithm for training neural networks, is typically understood as a symbolic computation that recursively applies the chain rule. We show it emerges exactly as the finite-time relaxation of a physical dynamical system. By formulating feedforward inference as a continuous-time process and applying Lagrangian theory of non-conservative systems to handle asymmetric interactions, we derive a global energy functional on a doubled state space encoding both activations and sensitivities. The saddle-point dynamics of this energy perform inference and credit assignment simultaneously through local interactions. We term this framework ''Dyadic Backpropagation''. Crucially, we prove that unit-step Euler discretization, the natural timescale of layer transitions, recovers standard backpropagation exactly in precisely 2L steps for an L-layer network, with no approximations. Unlike prior energy-based methods requiring symmetric weights, asymptotic convergence, or vanishing perturbations, our framework guarantees exact gradients in finite time. This establishes backpropagation as the digitally optimized shadow of a continuous physical relaxation, providing a rigorous foundation for exact gradient computation in analog and neuromorphic substrates where continuous dynamics are native.

### [SEDformer: Event-Synchronous Spiking Transformers for Irregular Telemetry Time Series Forecasting](http://arxiv.org/abs/2602.02230v2)
**2026-02-02** | *Ziyu Zhou, Yuchen Fang, Weilin Ruan et al.*

> Telemetry streams from large-scale Internet-connected systems (e.g., IoT deployments and online platforms) naturally form an irregular multivariate time series (IMTS) whose accurate forecasting is operationally vital. A closer examination reveals a defining Sparsity-Event Duality (SED) property of IMTS, i.e., long stretches with sparse or no observations are punctuated by short, dense bursts where most semantic events (observations) occur. However, existing Graph- and Transformer-based forecasters ignore SED: pre-alignment to uniform grids with heavy padding violates sparsity by inflating sequences and forcing computation at non-informative steps, while relational recasting weakens event semantics by disrupting local temporal continuity. These limitations motivate a more faithful and natural modeling paradigm for IMTS that aligns with its SED property. We find that Spiking Neural Networks meet this requirement, as they communicate via sparse binary spikes and update in an event-driven manner, aligning naturally with the SED nature of IMTS. Therefore, we present SEDformer, an SED-enhanced Spiking Transformer for telemetry IMTS forecasting that couples: (1) a SED-based Spike Encoder converts raw observations into event synchronous spikes using an Event-Aligned LIF neuron, (2) an Event-Preserving Temporal Downsampling module compresses long gaps while retaining salient firings and (3) a stack of SED-based Spike Transformer blocks enable intra-series dependency modeling with a membrane-based linear attention driven by EA-LIF spiking features. Experiments on public telemetry IMTS datasets show that SEDformer attains state-of-the-art forecasting accuracy while reducing energy and memory usage, providing a natural and efficient path for modeling IMTS.

### [Scale-covariant spiking wavelets](http://arxiv.org/abs/2602.02020v2)
**2026-02-02** | *Jens Egholm Pedersen, Tony Lindeberg, Peter Gerstoft*

> We establish a theoretical connection between wavelet transforms and spiking neural networks through scale-space theory. We rely on the scale-covariant guarantees in the leaky integrate-and-fire neurons to implement discrete mother wavelets that approximate continuous wavelets. A reconstruction experiment demonstrates the feasibility of the approach and warrants further analysis to mitigate current approximation errors. Our work suggests a novel spiking signal representation that could enable more energy-efficient signal processing algorithms.

### [SpikingGamma: Surrogate-Gradient Free and Temporally Precise Online Training of Spiking Neural Networks with Smoothed Delays](http://arxiv.org/abs/2602.01978v1)
**2026-02-02** | *Roel Koopman, Sebastian Otte, Sander Boht√©*

> Neuromorphic hardware implementations of Spiking Neural Networks (SNNs) promise energy-efficient, low-latency AI through sparse, event-driven computation. Yet, training SNNs under fine temporal discretization remains a major challenge, hindering both low-latency responsiveness and the mapping of software-trained SNNs to efficient hardware. In current approaches, spiking neurons are modeled as self-recurrent units, embedded into recurrent networks to maintain state over time, and trained with BPTT or RTRL variants based on surrogate gradients. These methods scale poorly with temporal resolution, while online approximations often exhibit instability for long sequences and tend to fail at capturing temporal patterns precisely. To address these limitations, we develop spiking neurons with internal recursive memory structures that we combine with sigma-delta spike-coding. We show that this SpikingGamma model supports direct error backpropagation without surrogate gradients, can learn fine temporal patterns with minimal spiking in an online manner, and scale feedforward SNNs to complex tasks and benchmarks with competitive accuracy, all while being insensitive to the temporal resolution of the model. Our approach offers both an alternative to current recurrent SNNs trained with surrogate gradients, and a direct route for mapping SNNs to neuromorphic hardware.

### [Governance at the Edge of Architecture: Regulating NeuroAI and Neuromorphic Systems](http://arxiv.org/abs/2602.01503v2)
**2026-02-02** | *Afifah Kashif, Abdul Muhsin Hameed, Asim Iqbal*

> Current AI governance frameworks, including regulatory benchmarks for accuracy, latency, and energy efficiency, are built for static, centrally trained artificial neural networks on von Neumann hardware. NeuroAI systems, embodied in neuromorphic hardware and implemented via spiking neural networks, break these assumptions. This paper examines the limitations of current AI governance frameworks for NeuroAI, arguing that assurance and audit methods must co-evolve with these architectures, aligning traditional regulatory metrics with the physics, learning dynamics, and embodied efficiency of brain-inspired computation to enable technically grounded assurance.

### [Parallel Training in Spiking Neural Networks](http://arxiv.org/abs/2602.01133v1)
**2026-02-01** | *Yanbin Huang, Man Yao, Yuqi Pan et al.*

> The bio-inspired integrate-fire-reset mechanism of spiking neurons constitutes the foundation for efficient processing in Spiking Neural Networks (SNNs). Recent progress in large models demands that spiking neurons support highly parallel computation to scale efficiently on modern GPUs. This work proposes a novel functional perspective that provides general guidance for designing parallel spiking neurons. We argue that the reset mechanism, which induces complex temporal dependencies and hinders parallel training, should be removed. However, any such modification should satisfy two principles: 1) preserving the functions of reset as a core biological mechanism; and 2) enabling parallel training without sacrificing the serial inference ability of spiking neurons, which underpins their efficiency at test time. To this end, we identify the functions of the reset and analyze how to reconcile parallel training with serial inference, upon which we propose a dynamic decay spiking neuron. We conduct comprehensive testing of our method in terms of: 1) Training efficiency and extrapolation capability. On 16k-length sequences, we achieve a 25.6x training speedup over the pioneering parallel spiking neuron, and our models trained on 2k-length can stably perform inference on sequences as long as 30k. 2) Generality. We demonstrate the consistent effectiveness of the proposed method across five task categories (image classification, neuromorphic event processing, time-series forecasting, language modeling, and reinforcement learning), three network architectures (spiking CNN/Transformer/SSMs), and two spike activation modes (spike/integer activation). 3) Energy consumption. The spiking firing of our neuron is lower than that of vanilla and existing parallel spiking neurons.

### [ChronoSpike: An Adaptive Spiking Graph Neural Network for Dynamic Graphs](http://arxiv.org/abs/2602.01124v1)
**2026-02-01** | *Md Abrar Jahin, Taufikur Rahman Fuad, Jay Pujara et al.*

> Dynamic graph representation learning requires capturing both structural relationships and temporal evolution, yet existing approaches face a fundamental trade-off: attention-based methods achieve expressiveness at $O(T^2)$ complexity, while recurrent architectures suffer from gradient pathologies and dense state storage. Spiking neural networks offer event-driven efficiency but remain limited by sequential propagation, binary information loss, and local aggregation that misses global context. We propose ChronoSpike, an adaptive spiking graph neural network that integrates learnable LIF neurons with per-channel membrane dynamics, multi-head attentive spatial aggregation on continuous features, and a lightweight Transformer temporal encoder, enabling both fine-grained local modeling and long-range dependency capture with linear memory complexity $O(T \cdot d)$. On three large-scale benchmarks, ChronoSpike outperforms twelve state-of-the-art baselines by $2.0\%$ Macro-F1 and $2.4\%$ Micro-F1 while achieving $3-10\times$ faster training than recurrent methods with a constant 105K-parameter budget independent of graph size. We provide theoretical guarantees for membrane potential boundedness, gradient flow stability under contraction factor $œÅ< 1$, and BIBO stability; interpretability analyses reveal heterogeneous temporal receptive fields and a learned primacy effect with $83-88\%$ sparsity.

---

## üëÅÔ∏è Applications & Sensing

### [Neuro-Inspired Visual Pattern Recognition via Biological Reservoir Computing](http://arxiv.org/abs/2602.05737v1)
**2026-02-05** | *Luca Ciampi, Ludovico Iannello, Fabrizio Tonelli et al.*

> In this paper, we present a neuro-inspired approach to reservoir computing (RC) in which a network of in vitro cultured cortical neurons serves as the physical reservoir. Rather than relying on artificial recurrent models to approximate neural dynamics, our biological reservoir computing (BRC) system leverages the spontaneous and stimulus-evoked activity of living neural circuits as its computational substrate. A high-density multi-electrode array (HD-MEA) provides simultaneous stimulation and readout across hundreds of channels: input patterns are delivered through selected electrodes, while the remaining ones capture the resulting high-dimensional neural responses, yielding a biologically grounded feature representation. A linear readout layer (single-layer perceptron) is then trained to classify these reservoir states, enabling the living neural network to perform static visual pattern-recognition tasks within a computer-vision framework. We evaluate the system across a sequence of tasks of increasing difficulty, ranging from pointwise stimuli to oriented bars, clock-digit-like shapes, and handwritten digits from the MNIST dataset. Despite the inherent variability of biological neural responses-arising from noise, spontaneous activity, and inter-session differences-the system consistently generates high-dimensional representations that support accurate classification. These results demonstrate that in vitro cortical networks can function as effective reservoirs for static visual pattern recognition, opening new avenues for integrating living neural substrates into neuromorphic computing frameworks. More broadly, this work contributes to the effort to incorporate biological principles into machine learning and supports the goals of neuro-inspired vision by illustrating how living neural systems can inform the design of efficient and biologically grounded computational models.

### [From Vision to Decision: Neuromorphic Control for Autonomous Navigation and Tracking](http://arxiv.org/abs/2602.05683v1)
**2026-02-05** | *Chuwei Wang, Eduardo Sebasti√°n, Amanda Prorok et al.*

> Robotic navigation has historically struggled to reconcile reactive, sensor-based control with the decisive capabilities of model-based planners. This duality becomes critical when the absence of a predominant option among goals leads to indecision, challenging reactive systems to break symmetries without computationally-intense planners. We propose a parsimonious neuromorphic control framework that bridges this gap for vision-guided navigation and tracking. Image pixels from an onboard camera are encoded as inputs to dynamic neuronal populations that directly transform visual target excitation into egocentric motion commands. A dynamic bifurcation mechanism resolves indecision by delaying commitment until a critical point induced by the environmental geometry. Inspired by recently proposed mechanistic models of animal cognition and opinion dynamics, the neuromorphic controller provides real-time autonomy with a minimal computational burden, a small number of interpretable parameters, and can be seamlessly integrated with application-specific image processing pipelines. We validate our approach in simulation environments as well as on an experimental quadrotor platform.

### [EventFlash: Towards Efficient MLLMs for Event-Based Vision](http://arxiv.org/abs/2602.03230v1)
**2026-02-03** | *Shaoyu Liu, Jianing Li, Guanghui Zhao et al.*

> Event-based multimodal large language models (MLLMs) enable robust perception in high-speed and low-light scenarios, addressing key limitations of frame-based MLLMs. However, current event-based MLLMs often rely on dense image-like processing paradigms, overlooking the spatiotemporal sparsity of event streams and resulting in high computational cost. In this paper, we propose EventFlash, a novel and efficient MLLM to explore spatiotemporal token sparsification for reducing data redundancy and accelerating inference. Technically, we build EventMind, a large-scale and scene-diverse dataset with over 500k instruction sets, providing both short and long event stream sequences to support our curriculum training strategy. We then present an adaptive temporal window aggregation module for efficient temporal sampling, which adaptively compresses temporal tokens while retaining key temporal cues. Finally, a sparse density-guided attention module is designed to improve spatial token efficiency by selecting informative regions and suppressing empty or sparse areas. Experimental results show that EventFlash achieves a $12.4\times$ throughput improvement over the baseline (EventFlash-Zero) while maintaining comparable performance. It supports long-range event stream processing with up to 1,000 bins, significantly outperforming the 5-bin limit of EventGPT. We believe EventFlash serves as an efficient foundation model for event-based vision.

---

## üìÇ General / Uncategorized

### [Online Vector Quantized Attention](http://arxiv.org/abs/2602.03922v1)
**2026-02-03** | *Nick Alonso, Tomas Figliolia, Beren Millidge*

> Standard sequence mixing layers used in language models struggle to balance efficiency and performance. Self-attention performs well on long context tasks but has expensive quadratic compute and linear memory costs, while linear attention and SSMs use only linear compute and constant memory but struggle with long context processing. In this paper, we develop a sequence mixing layer that aims to find a better compromise between memory-compute costs and long-context processing, which we call online vector-quantized (OVQ) attention. OVQ-attention requires linear compute costs and constant memory, but, unlike linear attention and SSMs, it uses a sparse memory update that allows it to greatly increase the size of its memory state and, consequently, memory capacity. We develop a theoretical basis for OVQ-attention based on Gaussian mixture regression, and we test it on a variety of synthetic long context tasks and on long context language modeling. OVQ-attention shows significant improvements over linear attention baselines and the original VQ-attention, on which OVQ-attention was inspired. It demonstrates competitive, and sometimes identical, performance to strong self-attention baselines up 64k sequence length, despite using a small fraction of the memory of full self-attention.

### [Dynamic Heuristic Neuromorphic Solver for the Edge User Allocation Problem with Bayesian Confidence Propagation Neural Network](http://arxiv.org/abs/2602.01294v1)
**2026-02-01** | *Kecheng Zhang, Anders Lansner, Ahsan Javed Awan et al.*

> We propose a neuromorphic solver for the NP-hard Edge User Allocation problem using an attractor network with Winner-Takes-All (WTA) mechanism implemented with the Bayesian Confidence Propagation Neural Network (BCPNN) framework. Unlike previous energy-based attractor networks, our solver uses dynamic heuristic biasing to guide allocations in real time and introduces a "no allocation" state to each WTA motif, achieving near-optimal performance with an empirically upper-bounded number of time steps. The approach is compatible with neuromorphic architectures and may offer improvements in energy efficiency.

---

