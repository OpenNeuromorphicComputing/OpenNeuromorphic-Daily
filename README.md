# ðŸ§  Open Neuromorphic - Daily ArXiv

**Automated Daily Update** | Last Run: 2026-02-14 08:31 UTC

Papers are automatically categorized by topic and sorted by date.

## ðŸ›  Hardware & Materials

### [Device-Circuit Co-Design of Variation-Resilient Read and Write Drivers for Antiferromagnetic Tunnel Junction (AFMTJ) Memories](http://arxiv.org/abs/2602.11614v1)
**2026-02-12** | *Yousuf Choudhary, Tosiron Adegbija*

> Antiferromagnetic Tunnel Junctions (AFMTJs) offer picosecond switching and high integration density for in-memory computing, but their ultrafast dynamics and low tunnel magnetoresistance (TMR) make state-of-the-art MRAM interfaces unreliable. This work develops a device-circuit co-designed read/write interface optimized for AFMTJ behavior. Using a calibrated SPICE AFMTJ model as a baseline, we identify the limitations of conventional drivers and propose an asymmetric pulse driver (PD) for deterministic picosecond switching and a self-timed sense amplifier (STSA) with dynamic trip-point tuning for low-TMR sensing. Our experiments using SPICE and Monte Carlo evaluations demonstrate that the proposed circuits preserve AFMTJ latency and energy benefits while achieving robust read/write yield under realistic PVT and 3D integration parasitics, outperforming standard MRAM front-ends under the same conditions.

### [Jamming-controlled stochasticity in metal-insulator switching](http://arxiv.org/abs/2602.11302v1)
**2026-02-11** | *NicolÃ² D'Anna, Nareg Ghazikhanian, Katherine Matthews et al.*

> Understanding and controlling phase transitions is a fundamental part of physics and has been central to many technological revolutions, from steam engines to field-effect transistors. At present, there is strong interest in materials with strongly coupled structural and electronic phase transitions, which hold promise for energy-efficient technologies. Utilizing a structural phase transition and controlling its plasticity naturally leads to built-in memory, a key feature for emulating neurons and synapses in neuromorphic technologies. Here, $\textit{operando}$ Bragg X-ray photon correlation spectroscopy is used to study the evolution of the nano-domain distribution at the micron-scale in neuromorphic devices made from the archetypal Mott insulator vanadium dioxide. It is found that after electrical switching, slow nano-domain reconfiguration occurs on timescales of thousands of seconds and that the domains undergo a jamming transition, offering control over switching stochasticity at the micron scale. More precisely, repetitive above-threshold currents plastically drive the system into a jammed/glassy state where switching becomes deterministic, while sub-threshold currents erase the short-term memory contained in the nano-domain distribution, recovering stochastic switching, thus offering a path for in-device learning. The results illustrate the importance of studying the nanoscale physics associated with phase transitions in strongly correlated materials, even for macroscopic devices, and offer guidance for future device operation schemes.

### [Amortized Inference of Neuron Parameters on Analog Neuromorphic Hardware](http://arxiv.org/abs/2602.10763v2)
**2026-02-11** | *Jakob Kaiser, Eric MÃ¼ller, Johannes Schemmel*

> Our work utilized a non-sequential simulation-based inference algorithm to provide an amortized neural density estimator, which approximates the posterior distribution for seven parameters of the adaptive exponential integrate-and-fire neuron model of the analog neuromorphic BrainScaleS-2 substrate. We constrained the large parameter space by training a binary classifier to predict parameter combinations yielding observations in regimes of interest, i.e. moderate spike counts. We compared two neural density estimators: one using handcrafted summary statistics and one using a summary network trained in combination with the neural density estimator. The summary network yielded a more focused posterior and generated posterior predictive traces that accurately captured the membrane potential dynamics. When using handcrafted summary statistics, posterior predictive traces match the included features but show deviations in the exact dynamics. The posteriors showed signs of bias and miscalibration but were still able to yield posterior predictive samples that were close to the target observations on which the posteriors were constrained. Our results validate amortized simulation-based inference as a tool for parameterizing analog neuron circuits.

### [Device Applications of Heterogeneously Integrated Strain-Switched Ferrimagnets/Topological Insulator/Piezoelectric Stacks](http://arxiv.org/abs/2602.10294v1)
**2026-02-10** | *Supriyo Bandyopadhyay*

> A family of ferrimagnets (CoV2O4, GdCo, TbCo) exhibits out-of-plane magnetic anisotropy when strained compressively and in-plane magnetic anisotropy when strained expansively (or vice versa). If such a ferrimagnetic thin film is placed on top of a topological insulator (TI) thin film and its magnetic anisotropy is modulated with strain, then interfacial exchange coupling between the ferrimagnet (FM) and the underlying TI will modulate the surface current flowing through the latter. If the strain is varied continuously, the current will also vary continuously and if the strain alternates in time, the current will also alternate with the frequency of the strain modulation, as long as the frequency is not so high that the period is smaller than the switching time of the FM. If the strain is generated with a gate voltage by integrating a piezoelectric underneath the FM/TI stack, then that can implement a transconductance amplifier or a synapse for neuromorphic computation.

### [Area-Efficient In-Memory Computing for Mixture-of-Experts via Multiplexing and Caching](http://arxiv.org/abs/2602.10254v1)
**2026-02-10** | *Hanyuan Gao, Xiaoxuan Yang*

> Mixture-of-Experts (MoE) layers activate a subset of model weights, dubbed experts, to improve model performance. MoE is particularly promising for deployment on process-in-memory (PIM) architectures, because PIM can naturally fit experts separately and provide great benefits for energy efficiency. However, PIM chips often suffer from large area overhead, especially in the peripheral circuits. In this paper, we propose an area-efficient in-memory computing architecture for MoE transformers. First, to reduce area, we propose a crossbar-level multiplexing strategy that exploits MoE sparsity: experts are deployed on crossbars and multiple crossbars share the same peripheral circuits. Second, we propose expert grouping and group-wise scheduling methods to alleviate the load imbalance and contention overhead caused by sharing. In addition, to address the problem that the expert choice router requires access to all hidden states during generation, we propose a gate-output (GO)cache to store necessary results and bypass expensive additional computation. Experiments show that our approaches improve the area efficiency of the MoE part by up to 2.2x compared to a SOTA architecture. During generation, the cache improves performance and energy efficiency by 4.2x and 10.1x, respectively, compared to the baseline when generating 8 tokens. The total performance density achieves 15.6 GOPS/W/mm2. The code is open source at https://github.com/superstarghy/MoEwithPIM.

### [Efficient Deep Learning for Biometrics: Overview, Challenges and Trends in Ear of Frugal AI](http://arxiv.org/abs/2602.08809v1)
**2026-02-09** | *Karim Haroun, Aya Zitouni, Aicha Zenakhri et al.*

> Recent advances in deep learning, whether on discriminative or generative tasks have been beneficial for various applications, among which security and defense. However, their increasing computational demands during training and deployment translates directly into high energy consumption. As a consequence, this induces a heavy carbon footprint which hinders their widespread use and scalability, but also a limitation when deployed on resource-constrained edge devices for real-time use. In this paper, we briefly survey efficient deep learning methods for biometric applications. Specifically, we tackle the challenges one might incur when training and deploying deep learning approaches, and provide a taxonomy of the various efficient deep learning families. Additionally, we discuss complementary metrics for evaluating the efficiency of these models such as memory, computation, latency, throughput, and advocate for universal and reproducible metrics for better comparison. Last, we give future research directions to consider.

### [2D ferroelectric narrow-bandgap semiconductor Wurtzite' type alpha-In2Se3 and its silicon-compatible growth](http://arxiv.org/abs/2602.08381v1)
**2026-02-09** | *Yuxuan Jiang, Xingkun Ning, Renhui Liu et al.*

> 2D van der Waals ferroelectrics, particularly alpha-In2Se3, have emerged as an attractive building block for next-generation information storage technologies due to their moderate band gap and robust ferroelectricity stabilized by dipole locking. alpha-In2Se3 can adopt either the distorted zincblende or wurtzite structures; however, the wurtzite phase has yet to be experimental-ly validated, and its large-scale synthesis poses significant challenges. Here, we report an in-situ transport growth of centimeter-scale wurtzite type alpha-In2Se3 films directly on SiO2 substrates using a process combining pulsed laser deposition and chemical vapor deposition. We demonstrate that it is a narrow bandgap ferroelectric semiconductor, featuring a Curie tem-perature exceeding 620 K, a tunable bandgap (0.8-1.6 eV) modulated by charged domain walls, and a large optical absorption coefficient of 1.3 times 10 powers 6 per centemeter. Moreover, light absorption promotes the dynamic conductance range, linearity, and symmetry of the synapse devices, leading to a high recognition accuracy of 92.3 percent in a supervised pattern classification task for neuromorphic computing. Our findings demonstrate a ferroelectric polymorphism of In2Se3, highlighting its potential in ferroelectric synapses for neuromorphic computing.

### [Antiferromagnetic Tunnel Junctions (AFMTJs) for In-Memory Computing: Modeling and Case Study](http://arxiv.org/abs/2602.08323v1)
**2026-02-09** | *Yousuf Choudhary, Tosiron Adegbija*

> Antiferromagnetic Tunnel Junctions (AFMTJs) enable picosecond switching and femtojoule writes through ultrafast sublattice dynamics. We present the first end-to-end AFMTJ simulation framework integrating multi-sublattice Landau-Lifshitz-Gilbert (LLG) dynamics with circuit-level modeling. SPICE-based simulations show that AFMTJs achieve ~8x lower write latency and ~9x lower write energy than conventional MTJs. When integrated into an in-memory computing architecture, AFMTJs deliver 17.5x average speedup and nearly 20x energy savings versus a CPU baseline-significantly outperforming MTJ-based IMC. These results establish AFMTJs as a compelling primitive for scalable, low-power computing.

### [Photonic neuromorphic processing with coupled spiking silicon microrings](http://arxiv.org/abs/2602.05918v1)
**2026-02-05** | *Giovanni Donati, Stefano Biasi, Lorenzo Pavesi et al.*

> Understanding the physical computing mechanisms of individual network nodes is essential for scaling neuromorphic photonic architectures. This work proposes a compact passive nonlinear photonic core based on a Side-Coupled Integrated Spaced Sequence of Resonators (SCISSOR) made of three nominally equal microrings and investigate its computing capabilities. Its nonlinearities and internal feedback enable analogue, spiking, and bistable responses that are accessed by tuning the injection power and wavelength. Implemented as a single nonlinear node in a time-multiplexed reservoir computing, the SCISSOR achieves error-free classification on the Iris dataset and accuracies above 97% on the Sonar task, using both analogue and digital reservoir representations with 150 virtual nodes. In the digital scheme, spiking dynamics naturally generate sparse reservoir states, enabling efficient classification even with a single spike. Intriguingly, optimal operating points are at the boundaries where sharp transitions in dynamical complexity and/or output power occur. In these points, the SCISSOR supports high task-performance, opening novel strategies for future on-chip training. Spiking and thermal bistabilities also participate to enhance the computational performance at low injected powers below 4 mW. These results suggest optical coupled microring resonators as effective building blocks for future edge computing and neuromorphic photonic systems.

### [Compact, Reconfigurable Optical Delay Line on a Bent Silica Fiber](http://arxiv.org/abs/2602.05757v1)
**2026-02-05** | *Manuel Crespo-Ballesteros, Misha Sumetsky*

> Tunable optical delay lines that simultaneously offer nanosecond-scale delay, broadband operation, low dispersion, and compact footprint remain challenging to realize with conventional integrated photonic platforms. Here we demonstrate a mechanically reconfigurable slow-light delay line based on a surface nanoscale axial photonics (SNAP) microresonator dynamically induced by controlled bending of a silica optical fiber. A localized nanoscale cutoff-wavelength dip, introduced by CO2-laser annealing, provides a reflective boundary, while fiber bending generates a smooth axial potential whose shape is continuously tunable via loop curvature. By adjusting the bending radius, the induced SNAP microresonator evolves from a nearly linear to an approximately semiparabolic axial profile, enabling a controlled transition from dispersive to nearly dispersionless delay. Using a transverse microfiber coupler operated at the impedance-matching condition, we experimentally demonstrate continuous delay tuning from 2 ns to 0.5 ns within a 10 GHz bandwidth in an approximately 2 mm long fiber segment, with insertion loss below 6 dB. The results confirm that mechanically induced SNAP microresonators provide a compact, robust, and reconfigurable platform for dispersion-engineered optical delay lines, with direct relevance to photonic beamforming, frequency-comb stabilization, and neuromorphic photonic signal processing.

### [Strong radial electric field scaling near nanoscale conductive filaments and the ReRAM resistive switching mechanism](http://arxiv.org/abs/2602.05067v1)
**2026-02-04** | *Robin Jacobs-Gedrim, William Wahby, Thomas Awe et al.*

> The physics underlying reset in bipolar resistive memory has been the subject of decades of controversy and has been identified as the primary barrier to resistive memory technology development. This manuscript introduces a nanoscale effect in current carrying conductors, whereby surface charge induced radial electric fields are found to be inversely proportional to the radius of the conductive path. This nanoscale effect is then applied to explain the negative resistance switching (reset) mechanism in filamentary metal oxide resistive switching memory devices (memristors). Previous explanations for the negative resistive switching mechanism state that diffusion constitutes the radial driving mechanism for oxygen ions, and drift under electric fields is restricted to the direction parallel to current flow. This explanation conflicts with retention and microscopy data collected in a subset of devices presented in literature. We demonstrate that the electric field's dependency on the on the radius of a nanoscale conductive path can result in radial fields on the order of 10^5 to 10^6 V/cm at only -1 V bias, sufficient to govern the negative resistance switching mechanism in filamentary metal oxides. By accounting for this nanoscale size effect, long standing anomalous experimental data about the negative (reset) resistance switching mechanism in bipolar filamentary resistive memory devices is finally reconciled. Wide understanding of surface charges and associated electric fields in nanoscale conductive paths could prove important for further scaling of integrated circuits and aid in elucidating many nanoscale phenomena.

### [Evolutionary Mapping of Neural Networks to Spatial Accelerators](http://arxiv.org/abs/2602.04717v1)
**2026-02-04** | *Alessandro Pierro, Jonathan Timcheck, Jason Yik et al.*

> Spatial accelerators, composed of arrays of compute-memory integrated units, offer an attractive platform for deploying inference workloads with low latency and low energy consumption. However, fully exploiting their architectural advantages typically requires careful, expert-driven mapping of computational graphs to distributed processing elements. In this work, we automate this process by framing the mapping challenge as a black-box optimization problem. We introduce the first evolutionary, hardware-in-the-loop mapping framework for neuromorphic accelerators, enabling users without deep hardware knowledge to deploy workloads more efficiently. We evaluate our approach on Intel Loihi 2, a representative spatial accelerator featuring 152 cores per chip in a 2D mesh. Our method achieves up to 35% reduction in total latency compared to default heuristics on two sparse multi-layer perceptron networks. Furthermore, we demonstrate the scalability of our approach to multi-chip systems and observe an up to 40% improvement in energy efficiency, without explicitly optimizing for it.

### [Scalable platform enabling reservoir computing with nanoporous oxide memristors for image recognition and time series prediction](http://arxiv.org/abs/2602.04619v1)
**2026-02-04** | *Joshua Donald, Ben A. Johnson, Amir Mehrnejat et al.*

> Typical mammal brains have some form of random connectivity between neurons. Reservoir computing, a neural network approach, uses random weights within its processing layer along with built-in recurrent connections and short-term, fading memory, and is shown to be time and training efficient in processing spatiotemporal signals. Here we prepared a niobium oxide-based thin film memristor device with intrinsic structural in-homogeneity in the form of random nanopores and performed computational tasks of XOR operations, image recognition, and time series prediction and reconstruction. For the latter task we chose a complex three-dimensional chaotic Lorenz-63 time series. By applying three temporal voltage waveforms individually across the device and training the readout layer with electrical current signals from a three-output physical reservoir, we achieved satisfactory prediction and reconstruction accuracy in comparison to the case of no reservoir. This work highlights the potential for scalable, on-chip devices using all-oxide reservoir systems, paving the way for energy-efficient neuromorphic electronics dealing with time signals.

### [Real-time processing of analog signals on accelerated neuromorphic hardware](http://arxiv.org/abs/2602.04582v1)
**2026-02-04** | *Yannik Stradmann, Johannes Schemmel, Mihai A. Petrovici et al.*

> Sensory processing with neuromorphic systems is typically done by using either event-based sensors or translating input signals to spikes before presenting them to the neuromorphic processor. Here, we offer an alternative approach: direct analog signal injection eliminates superfluous and power-intensive analog-to-digital and digital-to-analog conversions, making it particularly suitable for efficient near-sensor processing. We demonstrate this by using the accelerated BrainScaleS-2 mixed-signal neuromorphic research platform and interfacing it directly to microphones and a servo-motor-driven actuator. Utilizing BrainScaleS-2's 1000-fold acceleration factor, we employ a spiking neural network to transform interaural time differences into a spatial code and thereby predict the location of sound sources. Our primary contributions are the first demonstrations of direct, continuous-valued sensor data injection into the analog compute units of the BrainScaleS-2 ASIC, and actuator control using its embedded microprocessors. This enables a fully on-chip processing pipeline$\unicode{x2014}$from sensory input handling, via spiking neural network processing to physical action. We showcase this by programming the system to localize and align a servo motor with the spatial direction of transient noise peaks in real-time.

### [A Comparative Study of Digital Memristor-Based Processing-In-Memory from a Device and Reliability Perspective](http://arxiv.org/abs/2602.04035v1)
**2026-02-03** | *Thomas Neuner, Henriette Padberg, Lior Kornblum et al.*

> As data-intensive applications increasingly strain conventional computing systems, processing-in-memory (PIM) has emerged as a promising paradigm to alleviate the memory wall by minimizing data transfer between memory and processing units. This review presents the recent advances in both stateful and non-stateful logic techniques for PIM, focusing on emerging nonvolatile memory technologies such as resistive random-access memory (RRAM), phase-change memory (PCM), and magnetoresistive random-access memory (MRAM). Both experimentally demonstrated and simulated logic designs are critically examined, highlighting key challenges in reliability and the role of device-level optimization in enabling scalable and commercial viable PIM systems. The review begins with an overview of relevant logic families, memristive device types, and associated reliability metrics. Each logic family is then explored in terms of how it capitalizes on distinct device properties to implement logic techniques. A comparative table of representative device stacks and performance parameters illustrates trade-offs and quality indicators. Through this comprehensive analysis, the development of optimized, robust memristive devices for next-generation PIM applications is supported.

---

## ðŸ§  Algorithms & Theory

### [On the Sensitivity of Firing Rate-Based Federated Spiking Neural Networks to Differential Privacy](http://arxiv.org/abs/2602.12009v1)
**2026-02-12** | *Luiz Pereira, Mirko Perkusich, Dalton Valadares et al.*

> Federated Neuromorphic Learning (FNL) enables energy-efficient and privacy-preserving learning on devices without centralizing data. However, real-world deployments require additional privacy mechanisms that can significantly alter training signals. This paper analyzes how Differential Privacy (DP) mechanisms, specifically gradient clipping and noise injection, perturb firing-rate statistics in Spiking Neural Networks (SNNs) and how these perturbations are propagated to rate-based FNL coordination. On a speech recognition task under non-IID settings, ablations across privacy budgets and clipping bounds reveal systematic rate shifts, attenuated aggregation, and ranking instability during client selection. Moreover, we relate these shifts to sparsity and memory indicators. Our findings provide actionable guidance for privacy-preserving FNL, specifically regarding the balance between privacy strength and rate-dependent coordination.

### [Information Abstraction for Data Transmission Networks based on Large Language Models](http://arxiv.org/abs/2602.11022v1)
**2026-02-11** | *Haoyuan Zhu, Haonan Hu, Jie Zhang*

> Biological systems, particularly the human brain, achieve remarkable energy efficiency by abstracting information across multiple hierarchical levels. In contrast, modern artificial intelligence and communication systems often consume significant energy overheads in transmitting low-level data, with limited emphasis on abstraction. Despite its implicit importance, a formal and computational theory of information abstraction remains absent. In this work, we introduce the Degree of Information Abstraction (DIA), a general metric that quantifies how well a representation compresses input data while preserving task-relevant semantics. We derive a tractable information-theoretic formulation of DIA and propose a DIA-based information abstraction framework. As a case study, we apply DIA to a large language model (LLM)-guided video transmission task, where abstraction-aware encoding significantly reduces transmission volume by $99.75\%$, while maintaining semantic fidelity. Our results suggest that DIA offers a principled tool for rebalancing energy and information in intelligent systems and opens new directions in neural network design, neuromorphic computing, semantic communication, and joint sensing-communication architectures.

### [Nonlinear dynamics in magnonic Fabry-PÃ©rot resonators: Low-power neuron-like activation and transmission suppression](http://arxiv.org/abs/2602.10650v1)
**2026-02-11** | *Anton Lutsenko, Kevin G. Fripp, LukÃ¡Å¡ FlajÅ¡man et al.*

> We report on nonlinear spin-wave dynamics in magnonic Fabry-PÃ©rot resonators composed of yttrium iron garnet (YIG) films coupled to CoFeB nanostripes. Using super-Nyquist sampling magneto-optical Kerr effect microscopy and micromagnetic simulations, we observe a systematic downshift of the spin-wave transmission gaps as the excitation power increases. This nonlinear behavior occurs at low power levels, reduced by a strong spatial concentration of spin waves within the resonator. The resulting power-dependent transmission enables neuron-like activation behavior and frequency-selective nonlinear spin-wave absorption. Our results highlight magnonic Fabry-PÃ©rot resonators as compact low-power nonlinear elements for neuromorphic magnonic computing architectures.

### [UltraLIF: Fully Differentiable Spiking Neural Networks via Ultradiscretization and Max-Plus Algebra](http://arxiv.org/abs/2602.11206v1)
**2026-02-10** | *Jose Marie Antonio MiÃ±oza*

> Spiking Neural Networks (SNNs) offer energy-efficient, biologically plausible computation but suffer from non-differentiable spike generation, necessitating reliance on heuristic surrogate gradients. This paper introduces UltraLIF, a principled framework that replaces surrogate gradients with ultradiscretization, a mathematical formalism from tropical geometry providing continuous relaxations of discrete dynamics. The central insight is that the max-plus semiring underlying ultradiscretization naturally models neural threshold dynamics: the log-sum-exp function serves as a differentiable soft-maximum that converges to hard thresholding as a learnable temperature parameter $\eps \to 0$. Two neuron models are derived from distinct dynamical systems: UltraLIF from the LIF ordinary differential equation (temporal dynamics) and UltraDLIF from the diffusion equation modeling gap junction coupling across neuronal populations (spatial dynamics). Both yield fully differentiable SNNs trainable via standard backpropagation with no forward-backward mismatch. Theoretical analysis establishes pointwise convergence to classical LIF dynamics with quantitative error bounds and bounded non-vanishing gradients. Experiments on six benchmarks spanning static images, neuromorphic vision, and audio demonstrate improvements over surrogate gradient baselines, with gains most pronounced in single-timestep ($T{=}1$) settings on neuromorphic and temporal datasets. An optional sparsity penalty enables significant energy reduction while maintaining competitive accuracy.

### [Sparse Axonal and Dendritic Delays Enable Competitive SNNs for Keyword Classification](http://arxiv.org/abs/2602.09746v1)
**2026-02-10** | *Younes Bouhadjar, Emre Neftci*

> Training transmission delays in spiking neural networks (SNNs) has been shown to substantially improve their performance on complex temporal tasks. In this work, we show that learning either axonal or dendritic delays enables deep feedforward SNNs composed of leaky integrate-and-fire (LIF) neurons to reach accuracy comparable to existing synaptic delay learning approaches, while significantly reducing memory and computational overhead. SNN models with either axonal or dendritic delays achieve up to $95.58\%$ on the Google Speech Command (GSC) and $80.97\%$ on the Spiking Speech Command (SSC) datasets, matching or exceeding prior methods based on synaptic delays or more complex neuron models. By adjusting the delay parameters, we obtain improved performance for synaptic delay learning baselines, strengthening the comparison. We find that axonal delays offer the most favorable trade-off, combining lower buffering requirements with slightly higher accuracy than dendritic delays. We further show that the performance of axonal and dendritic delay models is largely preserved under strong delay sparsity, with as few as $20\%$ of delays remaining active, further reducing buffering requirements. Overall, our results indicate that learnable axonal and dendritic delays provide a resource-efficient and effective mechanism for temporal representation in SNNs. Code will be made available publicly upon acceptance. Code is available at https://github.com/YounesBouhadjar/AxDenSynDelaySNN

### [From Lightweight CNNs to SpikeNets: Benchmarking Accuracy-Energy Tradeoffs with Pruned Spiking SqueezeNet](http://arxiv.org/abs/2602.09717v1)
**2026-02-10** | *Radib Bin Kabir, Tawsif Tashwar Dipto, Mehedi Ahamed et al.*

> Spiking Neural Networks (SNNs) are increasingly studied as energy-efficient alternatives to Convolutional Neural Networks (CNNs), particularly for edge intelligence. However, prior work has largely emphasized large-scale models, leaving the design and evaluation of lightweight CNN-to-SNN pipelines underexplored. In this paper, we present the first systematic benchmark of lightweight SNNs obtained by converting compact CNN architectures into spiking networks, where activations are modeled with Leaky-Integrate-and-Fire (LIF) neurons and trained using surrogate gradient descent under a unified setup. We construct spiking variants of ShuffleNet, SqueezeNet, MnasNet, and MixNet, and evaluate them on CIFAR-10, CIFAR-100, and TinyImageNet, measuring accuracy, F1-score, parameter count, computational complexity, and energy consumption. Our results show that SNNs can achieve up to 15.7x higher energy efficiency than their CNN counterparts while retaining competitive accuracy. Among these, the SNN variant of SqueezeNet consistently outperforms other lightweight SNNs. To further optimize this model, we apply a structured pruning strategy that removes entire redundant modules, yielding a pruned architecture, SNN-SqueezeNet-P. This pruned model improves CIFAR-10 accuracy by 6% and reduces parameters by 19% compared to the original SNN-SqueezeNet. Crucially, it narrows the gap with CNN-SqueezeNet, achieving nearly the same accuracy (only 1% lower) but with an 88.1% reduction in energy consumption due to sparse spike-driven computations. Together, these findings establish lightweight SNNs as practical, low-power alternatives for edge deployment, highlighting a viable path toward deploying high-performance, low-power intelligence on the edge.

### [Kirin: Improving ANN efficiency with SNN Hybridization](http://arxiv.org/abs/2602.08817v1)
**2026-02-09** | *Chenyu Wang, Zhanglu Yan, Zhi Zhou et al.*

> Artificial neural networks (ANNs), particularly large language models (LLMs), demonstrate powerful inference capabilities but consume substantial energy. Conversely, spiking neural networks (SNNs) exhibit exceptional energy efficiency due to their binary and event-driven characteristics, thus motivating the study of ANN-to-SNN conversion. In this process, quantization plays a pivotal role, mapping LLMs' floating-point parameters to discrete SNN parameters via the temporal dimension of the time window. However, several challenges remain in the conversion process: (i) converting high bit-width quantization values into binary spikes requires longer time windows, increasing system latency; and (ii) the inherent trade-off between the information loss of single-spike schemes and the energy costs of multi-spike ones in SNN. To address these challenges, we propose Kirin, a integer and spike hybrid based SNN to achieve accuracy lossless ANN-to-SNN conversion with time and energy efficiency. Specifically, we first propose a Spike Matrix Hybridization strategy that encoding low bit-width parameters that leading to small time window size into binary spikes while preserving the rest in integer format, thereby reducing the overall latency of SNN execution. Second, we introduce a silence threshold mechanism to regulate the timing of single-spike firing, ensuring the output is mathematically equivalent to the LLM's output and preserves accuracy. Experimental results demonstrate that Kirin, under a W4A4\&8 quantization setting, achieves near-FP16 accuracy while reducing energy consumption by up to 84.66\% and shortening time steps by 93.75\%.

### [PTS-SNN: A Prompt-Tuned Temporal Shift Spiking Neural Networks for Efficient Speech Emotion Recognition](http://arxiv.org/abs/2602.08240v1)
**2026-02-09** | *Xun Su, Huamin Wang, Qi Zhang*

> Speech Emotion Recognition (SER) is widely deployed in Human-Computer Interaction, yet the high computational cost of conventional models hinders their implementation on resource-constrained edge devices. Spiking Neural Networks (SNNs) offer an energy-efficient alternative due to their event-driven nature; however, their integration with continuous Self-Supervised Learning (SSL) representations is fundamentally challenged by distribution mismatch, where high-dynamic-range embeddings degrade the information coding capacity of threshold-based neurons. To resolve this, we propose Prompt-Tuned Spiking Neural Networks (PTS-SNN), a parameter-efficient neuromorphic adaptation framework that aligns frozen SSL backbones with spiking dynamics. Specifically, we introduce a Temporal Shift Spiking Encoder to capture local temporal dependencies via parameter-free channel shifts, establishing a stable feature basis. To bridge the domain gap, we devise a Context-Aware Membrane Potential Calibration strategy. This mechanism leverages a Spiking Sparse Linear Attention module to aggregate global semantic context into learnable soft prompts, which dynamically regulate the bias voltages of Parametric Leaky Integrate-and-Fire (PLIF) neurons. This regulation effectively centers the heterogeneous input distribution within the responsive firing range, mitigating functional silence or saturation. Extensive experiments on five multilingual datasets (e.g., IEMOCAP, CASIA, EMODB) demonstrate that PTS-SNN achieves 73.34\% accuracy on IEMOCAP, comparable to competitive Artificial Neural Networks (ANNs), while requiring only 1.19M trainable parameters and 0.35 mJ inference energy per sample.

### [BadSNN: Backdoor Attacks on Spiking Neural Networks via Adversarial Spiking Neuron](http://arxiv.org/abs/2602.07200v1)
**2026-02-06** | *Abdullah Arafat Miah, Kevin Vu, Yu Bi*

> Spiking Neural Networks (SNNs) are energy-efficient counterparts of Deep Neural Networks (DNNs) with high biological plausibility, as information is transmitted through temporal spiking patterns. The core element of an SNN is the spiking neuron, which converts input data into spikes following the Leaky Integrate-and-Fire (LIF) neuron model. This model includes several important hyperparameters, such as the membrane potential threshold and membrane time constant. Both the DNNs and SNNs have proven to be exploitable by backdoor attacks, where an adversary can poison the training dataset with malicious triggers and force the model to behave in an attacker-defined manner. Yet, how an adversary can exploit the unique characteristics of SNNs for backdoor attacks remains underexplored. In this paper, we propose \textit{BadSNN}, a novel backdoor attack on spiking neural networks that exploits hyperparameter variations of spiking neurons to inject backdoor behavior into the model. We further propose a trigger optimization process to achieve better attack performance while making trigger patterns less perceptible. \textit{BadSNN} demonstrates superior attack performance on various datasets and architectures, as well as compared with state-of-the-art data poisoning-based backdoor attacks and robustness against common backdoor mitigation techniques. Codes can be found at https://github.com/SiSL-URI/BadSNN.

### [Sparse Spike Encoding of Channel Responses for Energy Efficient Human Activity Recognition](http://arxiv.org/abs/2602.06766v1)
**2026-02-06** | *Eleonora Cicciarella, Riccardo Mazzieri, Jacopo Pegoraro et al.*

> ISAC enables pervasive monitoring, but modern sensing algorithms are often too complex for energy-constrained edge devices. This motivates the development of learning techniques that balance accuracy performance and energy efficiency. Spiking Neural Networks (SNNs) are a promising alternative, processing information as sparse binary spike trains and potentially reducing energy consumption by orders of magnitude. In this work, we propose a spiking convolutional autoencoder (SCAE) that learns tailored spike-encoded representations of channel impulse responses (CIR), jointly trained with an SNN for human activity recognition (HAR), thereby eliminating the need for Doppler domain preprocessing. The results show that our SCAE-SNN achieves F1 scores comparable to a hybrid approach (almost 96%), while producing substantially sparser spike encoding (81.1% sparsity). We also show that encoding CIR data prior to classification improves both HAR accuracy and efficiency. The code is available at https://github.com/ele-ciccia/SCAE-SNN-HAR.

---

## ðŸ‘ï¸ Applications & Sensing

### [Energy-Aware Spike Budgeting for Continual Learning in Spiking Neural Networks for Neuromorphic Vision](http://arxiv.org/abs/2602.12236v1)
**2026-02-12** | *Anika Tabassum Meem, Muntasir Hossain Nadid, Md Zesun Ahmed Mia*

> Neuromorphic vision systems based on spiking neural networks (SNNs) offer ultra-low-power perception for event-based and frame-based cameras, yet catastrophic forgetting remains a critical barrier to deployment in continually evolving environments. Existing continual learning methods, developed primarily for artificial neural networks, seldom jointly optimize accuracy and energy efficiency, with particularly limited exploration on event-based datasets. We propose an energy-aware spike budgeting framework for continual SNN learning that integrates experience replay, learnable leaky integrate-and-fire neuron parameters, and an adaptive spike scheduler to enforce dataset-specific energy constraints during training. Our approach exhibits modality-dependent behavior: on frame-based datasets (MNIST, CIFAR-10), spike budgeting acts as a sparsity-inducing regularizer, improving accuracy while reducing spike rates by up to 47\%; on event-based datasets (DVS-Gesture, N-MNIST, CIFAR-10-DVS), controlled budget relaxation enables accuracy gains up to 17.45 percentage points with minimal computational overhead. Across five benchmarks spanning both modalities, our method demonstrates consistent performance improvements while minimizing dynamic power consumption, advancing the practical viability of continual learning in neuromorphic vision systems.

### [SynSacc: A Blender-to-V2E Pipeline for Synthetic Neuromorphic Eye-Movement Data and Sim-to-Real Spiking Model Training](http://arxiv.org/abs/2602.08726v1)
**2026-02-09** | *Khadija Iddrisu, Waseem Shariff, Suzanne Little et al.*

> The study of eye movements, particularly saccades and fixations, are fundamental to understanding the mechanisms of human cognition and perception. Accurate classification of these movements requires sensing technologies capable of capturing rapid dynamics without distortion. Event cameras, also known as Dynamic Vision Sensors (DVS), provide asynchronous recordings of changes in light intensity, thereby eliminating motion blur inherent in conventional frame-based cameras and offering superior temporal resolution and data efficiency. In this study, we introduce a synthetic dataset generated with Blender to simulate saccades and fixations under controlled conditions. Leveraging Spiking Neural Networks (SNNs), we evaluate its robustness by training two architectures and finetuning on real event data. The proposed models achieve up to 0.83 accuracy and maintain consistent performance across varying temporal resolutions, demonstrating stability in eye movement classification. Moreover, the use of SNNs with synthetic event streams yields substantial computational efficiency gains over artificial neural network (ANN) counterparts, underscoring the utility of synthetic data augmentation in advancing event-based vision. All code and datasets associated with this work is available at https: //github.com/Ikhadija-5/SynSacc-Dataset.

### [A neuromorphic model of the insect visual system for natural image processing](http://arxiv.org/abs/2602.06405v1)
**2026-02-06** | *Adam D. Hines, Karin NordstrÃ¶m, Andrew B. Barron*

> Insect vision supports complex behaviors including associative learning, navigation, and object detection, and has long motivated computational models for understanding biological visual processing. However, many contemporary models prioritize task performance while neglecting biologically grounded processing pathways. Here, we introduce a bio-inspired vision model that captures principles of the insect visual system to transform dense visual input into sparse, discriminative codes. The model is trained using a fully self-supervised contrastive objective, enabling representation learning without labeled data and supporting reuse across tasks without reliance on domain-specific classifiers. We evaluated the resulting representations on flower recognition tasks and natural image benchmarks. The model consistently produced reliable sparse codes that distinguish visually similar inputs. To support different modelling and deployment uses, we have implemented the model as both an artificial neural network and a spiking neural network. In a simulated localization setting, our approach outperformed a simple image downsampling comparison baseline, highlighting the functional benefit of incorporating neuromorphic visual processing pathways. Collectively, these results advance insect computational modelling by providing a generalizable bio-inspired vision model capable of sparse computation across diverse tasks.

### [Neuro-Inspired Visual Pattern Recognition via Biological Reservoir Computing](http://arxiv.org/abs/2602.05737v1)
**2026-02-05** | *Luca Ciampi, Ludovico Iannello, Fabrizio Tonelli et al.*

> In this paper, we present a neuro-inspired approach to reservoir computing (RC) in which a network of in vitro cultured cortical neurons serves as the physical reservoir. Rather than relying on artificial recurrent models to approximate neural dynamics, our biological reservoir computing (BRC) system leverages the spontaneous and stimulus-evoked activity of living neural circuits as its computational substrate. A high-density multi-electrode array (HD-MEA) provides simultaneous stimulation and readout across hundreds of channels: input patterns are delivered through selected electrodes, while the remaining ones capture the resulting high-dimensional neural responses, yielding a biologically grounded feature representation. A linear readout layer (single-layer perceptron) is then trained to classify these reservoir states, enabling the living neural network to perform static visual pattern-recognition tasks within a computer-vision framework. We evaluate the system across a sequence of tasks of increasing difficulty, ranging from pointwise stimuli to oriented bars, clock-digit-like shapes, and handwritten digits from the MNIST dataset. Despite the inherent variability of biological neural responses-arising from noise, spontaneous activity, and inter-session differences-the system consistently generates high-dimensional representations that support accurate classification. These results demonstrate that in vitro cortical networks can function as effective reservoirs for static visual pattern recognition, opening new avenues for integrating living neural substrates into neuromorphic computing frameworks. More broadly, this work contributes to the effort to incorporate biological principles into machine learning and supports the goals of neuro-inspired vision by illustrating how living neural systems can inform the design of efficient and biologically grounded computational models.

### [From Vision to Decision: Neuromorphic Control for Autonomous Navigation and Tracking](http://arxiv.org/abs/2602.05683v1)
**2026-02-05** | *Chuwei Wang, Eduardo SebastiÃ¡n, Amanda Prorok et al.*

> Robotic navigation has historically struggled to reconcile reactive, sensor-based control with the decisive capabilities of model-based planners. This duality becomes critical when the absence of a predominant option among goals leads to indecision, challenging reactive systems to break symmetries without computationally-intense planners. We propose a parsimonious neuromorphic control framework that bridges this gap for vision-guided navigation and tracking. Image pixels from an onboard camera are encoded as inputs to dynamic neuronal populations that directly transform visual target excitation into egocentric motion commands. A dynamic bifurcation mechanism resolves indecision by delaying commitment until a critical point induced by the environmental geometry. Inspired by recently proposed mechanistic models of animal cognition and opinion dynamics, the neuromorphic controller provides real-time autonomy with a minimal computational burden, a small number of interpretable parameters, and can be seamlessly integrated with application-specific image processing pipelines. We validate our approach in simulation environments as well as on an experimental quadrotor platform.

---

